diff --git a/.gitignore b/.gitignore
index 7da4c87..afaf347 100644
--- a/.gitignore
+++ b/.gitignore
@@ -22,4 +22,13 @@ Thumbs.db
 
 # Build artifacts
 *.log
-coverage/
\ No newline at end of file
+coverage/
+
+# Local build directories
+build/
+packages/core/build/
+# python build outputs
+packages/core/build/
+dist/
+*.egg-info/
+
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
index 0f8c214..1fdc6ec 100644
--- a/.pre-commit-config.yaml
+++ b/.pre-commit-config.yaml
@@ -3,5 +3,13 @@ repos:
     rev: 24.4.2
     hooks: [{id: black}]
   - repo: https://github.com/astral-sh/ruff-pre-commit
-    rev: v0.5.6
-    hooks: [{id: ruff}]
\ No newline at end of file
+    rev: v0.5.5
+    hooks:
+      - id: ruff
+        args: ["--fix", "--exit-non-zero-on-fix"]
+        exclude: |
+          (?x)(
+            ^packages/core/build/|
+            ^dist/|
+            \.egg-info/
+          )
\ No newline at end of file
diff --git a/apps/web/src/App.tsx b/apps/web/src/App.tsx
index 2c58bd7..703ded3 100644
--- a/apps/web/src/App.tsx
+++ b/apps/web/src/App.tsx
@@ -2,17 +2,15 @@ import "./App.css";
 import Analysis from "./pages/Analysis";
 import Portfolio from "./pages/Portfolio";
 import Company from "./pages/Company";
-import { BrowserRouter, Routes, Route, Navigate } from "react-router-dom";
+import { Routes, Route, Navigate } from "react-router-dom";
 
 export default function App() {
   return (
-    <BrowserRouter>
-      <Routes>
-        <Route path="/" element={<Navigate to="/analysis" replace />} />
-        <Route path="/analysis" element={<Analysis />} />
-        <Route path="/portfolio" element={<Portfolio />} />
-        <Route path="/company/:symbol" element={<Company />} />
-      </Routes>
-    </BrowserRouter>
+    <Routes>
+      <Route path="/" element={<Navigate to="/analysis" replace />} />
+      <Route path="/analysis" element={<Analysis />} />
+      <Route path="/portfolio" element={<Portfolio />} />
+      <Route path="/company/:symbol" element={<Company />} />
+    </Routes>
   );
 }
\ No newline at end of file
diff --git a/packages/core/build/lib/core/__init__.py b/packages/core/build/lib/core/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/packages/core/build/lib/core/_legacy_data_fetch.py b/packages/core/build/lib/core/_legacy_data_fetch.py
new file mode 100644
index 0000000..18b1cc0
--- /dev/null
+++ b/packages/core/build/lib/core/_legacy_data_fetch.py
@@ -0,0 +1,425 @@
+import os
+import requests
+import zipfile
+import io
+import pandas as pd
+import time
+import aiohttp
+import asyncio
+import logging
+import numpy as np
+from xml.etree.ElementTree import parse
+from dotenv import load_dotenv
+from datetime import datetime, timedelta
+import json
+
+# --- ê¸°ë³¸ ì„¤ì • ---
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
+)
+load_dotenv()
+API_KEY = os.getenv("API_KEY")
+APP_KEY = os.getenv("APP_KEY")
+APP_SECRET = os.getenv("APP_SECRET")
+
+if not all([API_KEY, APP_KEY, APP_SECRET]):
+    raise ValueError("API í‚¤ê°€ .env íŒŒì¼ì— ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
+
+CACHE_DIR = "data/cache"
+KIS_BASE_URL = "https://openapi.koreainvestment.com:9443"
+KIS_ACCESS_TOKEN_CACHE = {"token": None, "expires_at": None}
+os.makedirs(CACHE_DIR, exist_ok=True)
+
+
+# --- ìºì‹± í—¬í¼ í•¨ìˆ˜ ---
+def _get_cache_path(cache_type, *args):
+    return os.path.join(CACHE_DIR, cache_type, *[str(a) for a in args])
+
+
+def _is_cache_valid(file_path, duration_days=0):
+    if not os.path.exists(file_path):
+        return False
+    file_mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
+    return (datetime.now() - file_mod_time) < timedelta(days=duration_days)
+
+
+def _json_default(o):
+    """numpy ìˆ«ìë¥¼ íŒŒì´ì¬ ê¸°ë³¸í˜•ìœ¼ë¡œ ë³€í™˜"""
+    if isinstance(o, (np.integer,)):
+        return int(o)
+    if isinstance(o, (np.floating,)):
+        return float(o)
+    if isinstance(o, (np.ndarray,)):
+        return o.tolist()
+    raise TypeError(f"{type(o)} is not JSON serializable")
+
+
+def _save_data_to_cache(data, file_path):
+    os.makedirs(os.path.dirname(file_path), exist_ok=True)
+    try:
+        if isinstance(data, pd.DataFrame):
+            data.to_parquet(file_path, index=False)
+        elif isinstance(data, dict):
+            with open(file_path, "w", encoding="utf-8") as f:
+                json.dump(data, f, ensure_ascii=False, indent=4, default=_json_default)
+        logging.info(f"âœ… ë°ì´í„° ìºì‹œ ì €ì¥: {file_path}")
+    except Exception as e:
+        logging.error(f"âŒ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ ({file_path}): {e}")
+
+
+def _load_data_from_cache(file_path):
+    try:
+        if file_path.endswith(".parquet"):
+            return pd.read_parquet(file_path)
+        elif file_path.endswith(".json"):
+            with open(file_path, "r", encoding="utf-8") as f:
+                return json.load(f)
+    except Exception:
+        return None
+
+
+# --- KIS API ê³µí†µ í•¨ìˆ˜ ---
+async def _get_kis_access_token_async(session):
+    if (
+        KIS_ACCESS_TOKEN_CACHE["token"]
+        and datetime.now() < KIS_ACCESS_TOKEN_CACHE["expires_at"]
+    ):
+        return KIS_ACCESS_TOKEN_CACHE["token"]
+    url = f"{KIS_BASE_URL}/oauth2/tokenP"
+    body = {
+        "grant_type": "client_credentials",
+        "appkey": APP_KEY,
+        "appsecret": APP_SECRET,
+    }
+    try:
+        async with session.post(
+            url,
+            headers={"Content-Type": "application/json"},
+            data=json.dumps(body),
+            timeout=10,
+        ) as resp:
+            resp.raise_for_status()
+            token_data = await resp.json()
+            if "access_token" in token_data:
+                token = token_data["access_token"]
+                expires_at = datetime.now() + timedelta(
+                    seconds=token_data.get("expires_in", 86400) - 300
+                )
+                KIS_ACCESS_TOKEN_CACHE.update(
+                    {"token": token, "expires_at": expires_at}
+                )
+                return token
+    except Exception as e:
+        logging.error(f"âŒ KIS ì•¡ì„¸ìŠ¤ í† í° ë°œê¸‰ ì‹¤íŒ¨: {e}")
+    return None
+
+
+async def _fetch_kis_data(session, url, tr_id, params):
+    token = await _get_kis_access_token_async(session)
+    if not token:
+        return None
+    headers = {
+        "Authorization": f"Bearer {token}",
+        "appkey": APP_KEY,
+        "appsecret": APP_SECRET,
+        "tr_id": tr_id,
+        "custtype": "P",
+    }
+    try:
+        async with session.get(url, headers=headers, params=params, timeout=10) as resp:
+            resp.raise_for_status()
+            data = await resp.json()
+            if data.get("rt_cd") == "0":
+                return data
+            else:
+                logging.warning(
+                    f"KIS API ì¡°íšŒ ì‹¤íŒ¨ ({tr_id} / {params.get('fid_input_iscd')}): {data.get('msg1')}"
+                )
+    except Exception as e:
+        logging.error(f"âŒ KIS API ìš”ì²­ ì˜¤ë¥˜ ({tr_id}): {e}")
+    return None
+
+
+# --- ë°ì´í„° ì†ŒìŠ¤ë³„ ì¡°íšŒ í•¨ìˆ˜ ---
+
+
+def load_or_create_corp_code_list():
+    filename = _get_cache_path("corp_codes", "corp_code_list.parquet")
+    if _is_cache_valid(filename, duration_days=1):
+        cached_df = _load_data_from_cache(filename)
+        if cached_df is not None:
+            return cached_df
+    try:
+        dart_url = f"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={API_KEY}"
+        res = requests.get(dart_url)
+        res.raise_for_status()
+        with zipfile.ZipFile(io.BytesIO(res.content)) as z, z.open("CORPCODE.xml") as f:
+            tree = parse(f)
+        root = tree.getroot()
+        data = [
+            (
+                c.find("corp_code").text,
+                c.find("corp_name").text,
+                c.find("stock_code").text.strip(),
+            )
+            for c in root.findall("list")
+            if c.find("stock_code").text.strip()
+        ]
+        dart_df = pd.DataFrame(data, columns=["corp_code", "corp_name", "stock_code"])
+        dart_df["stock_code"] = dart_df["stock_code"].str.zfill(6)
+
+        otp_url = "http://data.krx.co.kr/comm/fileDn/GenerateOTP/generate.cmd"
+        otp_form = {
+            "mktId": "ALL",
+            "trdDd": pd.Timestamp.today().strftime("%Y%m%d"),
+            "url": "dbms/MDC/STAT/standard/MDCSTAT01901",
+        }
+        otp_res = requests.post(
+            otp_url, data=otp_form, headers={"User-Agent": "Mozilla/5.0"}
+        )
+        otp_res.raise_for_status()
+
+        download_url = "http://data.krx.co.kr/comm/fileDn/download_csv/download.cmd"
+        krx_res = requests.post(
+            download_url,
+            data={"code": otp_res.text},
+            headers={"User-Agent": "Mozilla/5.0"},
+        )
+        krx_res.raise_for_status()
+
+        krx_df = pd.read_csv(io.BytesIO(krx_res.content), encoding="euc-kr")
+        code_col = next(
+            (col for col in ["ì¢…ëª©ì½”ë“œ", "ë‹¨ì¶•ì½”ë“œ"] if col in krx_df.columns), None
+        )
+        if not code_col:
+            raise KeyError("KRX ë°ì´í„°ì—ì„œ ì¢…ëª©ì½”ë“œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
+
+        krx_df.rename(
+            columns={
+                code_col: "stock_code",
+                "ê¸°ì—…ëª…": "corp_name_krx",
+                "ì‹œì¥êµ¬ë¶„": "market",
+            },
+            inplace=True,
+        )
+        krx_df["stock_code"] = krx_df["stock_code"].astype(str).str.zfill(6)
+
+        merged_df = pd.merge(
+            dart_df, krx_df[["stock_code", "market"]], on="stock_code", how="inner"
+        )
+        _save_data_to_cache(merged_df, filename)
+        return merged_df
+    except Exception as e:
+        logging.error(f"âŒ ìµœì‹  ìƒì¥ì‚¬ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}", exc_info=True)
+        return pd.DataFrame()
+
+
+async def get_financial_statement_async(
+    session, corp_code, year, return_report_name=False
+):
+    report_configs = [
+        ("11011", "ì‚¬ì—…ë³´ê³ ì„œ"),
+        ("11014", "3ë¶„ê¸°ë³´ê³ ì„œ"),
+        ("11012", "ë°˜ê¸°ë³´ê³ ì„œ"),
+        ("11013", "1ë¶„ê¸°ë³´ê³ ì„œ"),
+    ]
+    fs_configs = [("CFS", "ì—°ê²°"), ("OFS", "ë³„ë„")]
+    for rp_code, rp_name in report_configs:
+        for fs_div, fs_name in fs_configs:
+            cache_file = _get_cache_path(
+                "financials", corp_code, f"async_{year}_{rp_code}_{fs_div}.parquet"
+            )
+            if _is_cache_valid(cache_file, duration_days=7):
+                df = _load_data_from_cache(cache_file)
+                if df is not None:
+                    result = (
+                        corp_code,
+                        {"status": "000", "list": df.to_dict(orient="records")},
+                    )
+                    return (
+                        (*result, f"{rp_name} - {fs_name}")
+                        if return_report_name
+                        else result
+                    )
+            url = "https://opendart.fss.or.kr/api/fnlttSinglAcntAll.json"
+            params = {
+                "crtfc_key": API_KEY,
+                "corp_code": corp_code,
+                "bsns_year": str(year),
+                "reprt_code": rp_code,
+                "fs_div": fs_div,
+            }
+            try:
+                await asyncio.sleep(0.1)
+                async with session.get(url, params=params, timeout=10) as resp:
+                    resp.raise_for_status()
+                    data = await resp.json()
+                    if data.get("status") == "000" and data.get("list"):
+                        df = pd.DataFrame(data["list"])
+                        _save_data_to_cache(df, cache_file)
+                        result = (corp_code, data)
+                        return (
+                            (*result, f"{rp_name} - {fs_name}")
+                            if return_report_name
+                            else result
+                        )
+            except Exception as e:
+                logging.error(f"DART API ì˜¤ë¥˜ ({corp_code}, {year}, {rp_name}): {e}")
+                error_result = (corp_code, {"status": "999", "message": str(e)}, None)
+                return error_result if return_report_name else error_result[:2]
+
+    logging.warning(f"âŒ {corp_code}ì˜ {year}ë…„ ì¬ë¬´ì œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
+    final_result = (corp_code, {"status": "013", "message": "ë°ì´í„° ì—†ìŒ"})
+    return (*final_result, "N/A") if return_report_name else final_result
+
+
+async def get_kis_daily_price_async(session, stock_code, start_date, end_date):
+    start_dt, end_dt = pd.to_datetime(start_date), pd.to_datetime(end_date)
+    cache_file = _get_cache_path(
+        "prices",
+        stock_code,
+        f'kis_{start_dt.strftime("%Y%m%d")}_{end_dt.strftime("%Y%m%d")}.parquet',
+    )
+    if _is_cache_valid(cache_file, duration_days=1):
+        df = _load_data_from_cache(cache_file)
+        if df is not None:
+            return stock_code, df
+
+    url = f"{KIS_BASE_URL}/uapi/domestic-stock/v1/quotations/inquire-daily-price"
+    params = {
+        "FID_COND_MRKT_DIV_CODE": "J",
+        "FID_INPUT_ISCD": stock_code,
+        "FID_INPUT_DATE_1": start_dt.strftime("%Y%m%d"),
+        "FID_INPUT_DATE_2": end_dt.strftime("%Y%m%d"),
+        "FID_PERIOD_DIV_CODE": "D",
+        "FID_ORG_ADJ_PRC": "1",
+    }
+    data = await _fetch_kis_data(session, url, "FHKST01010400", params)
+
+    if data and data.get("output"):
+        df = pd.DataFrame(data["output"])
+        df.rename(
+            columns={
+                "stck_bsop_date": "date",
+                "stck_oprc": "open",
+                "stck_hgpr": "high",
+                "stck_lwpr": "low",
+                "stck_clpr": "close",
+                "acml_vol": "volume",
+                "acml_tr_pbmn": "transaction_amount",
+                "prdy_vrss": "change",
+            },
+            inplace=True,
+        )
+        final_cols = [
+            col
+            for col in [
+                "date",
+                "open",
+                "high",
+                "low",
+                "close",
+                "volume",
+                "transaction_amount",
+                "change",
+            ]
+            if col in df.columns
+        ]
+        for col in final_cols:
+            if col != "date":
+                df[col] = pd.to_numeric(df[col], errors="coerce")
+        df["date"] = pd.to_datetime(df["date"], format="%Y%m%d")
+        df = df[final_cols].sort_values("date").reset_index(drop=True)
+        _save_data_to_cache(df, cache_file)
+        return stock_code, df
+
+    return stock_code, pd.DataFrame()
+
+
+# [ì‹ ê·œ] KIS API ìƒì„¸ ì¬ë¬´ë¹„ìœ¨ ì¡°íšŒ
+async def get_kis_financial_ratios_async(session, stock_code: str):
+    cache_file = _get_cache_path("kis_ratios", f"{stock_code}.parquet")
+    if _is_cache_valid(cache_file, duration_days=7):
+        df = _load_data_from_cache(cache_file)
+        if df is not None:
+            return df
+
+    url = f"{KIS_BASE_URL}/uapi/domestic-stock/v1/finance/financial-ratio"
+    params = {
+        "fid_div_cls_code": "0",  # 0:ì—°ê°„, 1:ë¶„ê¸° (í•„ìˆ˜)
+        "fid_cond_mrkt_div_code": "J",
+        "fid_input_iscd": stock_code,
+    }
+    data = await _fetch_kis_data(session, url, "FHKST66430300", params)
+    if data and data.get("output"):
+        df = pd.DataFrame(data["output"])
+        cols = {
+            "stac_yymm": "ê²°ì‚°ë…„ì›”",
+            "grs_rt": "ë§¤ì¶œì´ì´ìµë¥ ",
+            "bsop_prfi_inrt": "ì˜ì—…ì´ìµë¥ ",
+            "thtr_ntin_inrt": "ë‹¹ê¸°ìˆœì´ìµë¥ ",
+            "roe_val": "ROE",
+            "eps": "EPS",
+            "bps": "BPS",
+            "pbr": "PBR",
+            "dvd_yd_rt": "ë°°ë‹¹ìˆ˜ìµë¥ ",
+        }
+        df = df[[key for key in cols.keys() if key in df.columns]].rename(columns=cols)
+        for col in df.columns:
+            if col != "ê²°ì‚°ë…„ì›”":
+                df[col] = pd.to_numeric(df[col], errors="coerce")
+        _save_data_to_cache(df, cache_file)
+        return df
+    return pd.DataFrame()
+
+
+# [ì‹ ê·œ] KIS API íˆ¬ìì˜ê²¬ ì¡°íšŒ
+async def get_kis_investment_opinion_async(session, stock_code: str):
+    cache_file = _get_cache_path("kis_opinion", f"{stock_code}.json")
+    if _is_cache_valid(cache_file, duration_days=1):
+        data = _load_data_from_cache(cache_file)
+        if data:
+            return data
+
+    # âœ… ë¬¸ì„œ ê¸°ì¤€ ìµœì‹  ê²½ë¡œâ€†/â€†TR_ID ì‚¬ìš©
+    url = f"{KIS_BASE_URL}/uapi/domestic-stock/v1/quotations/invest-opinion"
+    params = {
+        "FID_COND_MRKT_DIV_CODE": "J",  # ì‹œì¥êµ¬ë¶„
+        "FID_COND_SCR_DIV_CODE": "16633",  # ê³ ì •ê°’(ë¬¸ì„œ PK)
+        "FID_INPUT_ISCD": stock_code,
+        "FID_INPUT_DATE_1": (datetime.today() - timedelta(days=365)).strftime("%Y%m%d"),
+        "FID_INPUT_DATE_2": datetime.today().strftime("%Y%m%d"),
+    }
+    data = await _fetch_kis_data(session, url, "FHKST663300C0", params)
+
+    if data and data.get("output"):
+        output = data["output"][0]
+
+        opinion_data = {
+            "opinion": output.get("invt_opnn"),  # ex) ë§¤ìˆ˜, ì¤‘ë¦½
+            "target_price": float(output.get("hts_goal_prc") or 0),  # numpy â†’ float
+            "analyst_count": int(output.get("nm_of_analyst") or 0),  # ë¹ˆ ê°’ â†’ 0
+        }
+        _save_data_to_cache(opinion_data, cache_file)
+        return opinion_data
+    return {}
+
+
+# --- ì—¬ëŸ¬ ê¸°ì—… ë°ì´í„° ë™ì‹œ ì¡°íšŒ ---
+async def fetch_multiple_financials(corp_codes, year):
+    async with aiohttp.ClientSession() as session:
+        tasks = [
+            get_financial_statement_async(session, code, year) for code in corp_codes
+        ]
+        results = await asyncio.gather(*tasks)
+        return {code: data for code, data in results}
+
+
+async def fetch_multiple_prices(stock_codes, start_date, end_date):
+    async with aiohttp.ClientSession() as session:
+        tasks = [
+            get_kis_daily_price_async(session, code, start_date, end_date)
+            for code in stock_codes
+        ]
+        results = await asyncio.gather(*tasks)
+        return {code: df for code, df in results if df is not None}
diff --git a/packages/core/build/lib/core/_legacy_report_generator.py b/packages/core/build/lib/core/_legacy_report_generator.py
new file mode 100644
index 0000000..fbc6ce5
--- /dev/null
+++ b/packages/core/build/lib/core/_legacy_report_generator.py
@@ -0,0 +1,479 @@
+import os
+import requests
+import tempfile
+import pandas as pd
+import numpy as np  # np.nan ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€
+from fpdf import FPDF
+import plotly.express as px
+import plotly.graph_objects as go
+import logging
+from io import BytesIO
+from dotenv import load_dotenv  # API_KEY ë¡œë“œë¥¼ ìœ„í•´ ì¶”ê°€ (ë…ë¦½ ì‹¤í–‰ì‹œ)
+
+# src í´ë” ë‚´ ëª¨ë“ˆ ì„í¬íŠ¸
+from ._legacy_analysis import extract_fs_summary
+from ._legacy_data_fetch import (
+    load_or_create_corp_code_list,
+    get_fiscal_month,
+)  # get_fiscal_monthëŠ” í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, í˜¹ì‹œ ëª¨ë¥¼ ë¯¸ë˜ í™•ì¥ì„ ìœ„í•´ ìœ ì§€
+
+# ë¡œê¹… ì„¤ì • (ë‹¤ë¥¸ ëª¨ë“ˆê³¼ ì¼ê´€ì„± ìœ ì§€)
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
+)
+
+# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (ì´ ëª¨ë“ˆì´ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ë  ê²½ìš°ë¥¼ ëŒ€ë¹„)
+load_dotenv()
+API_KEY = os.getenv("API_KEY")
+if not API_KEY:
+    logging.warning(
+        "API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ data_fetch ëª¨ë“ˆì´ ë¨¼ì € ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
+    )
+
+# ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ ìƒì„±
+os.makedirs("data/reports", exist_ok=True)
+os.makedirs("data/fonts", exist_ok=True)  # í°íŠ¸ í´ë” ìƒì„±
+
+
+# ---------------------------------------------------
+# PDF ìƒì„± í´ë˜ìŠ¤ í™•ì¥
+# ---------------------------------------------------
+class PDFReport(FPDF):
+    def __init__(self, orientation="P", unit="mm", format="A4"):
+        super().__init__(orientation, unit, format)
+        self.set_auto_page_break(auto=True, margin=15)
+        self.add_korean_font()  # í•œê¸€ í°íŠ¸ ì¶”ê°€
+        self.set_font("CustomFont", "", 12)  # ê¸°ë³¸ í°íŠ¸ ì„¤ì •
+
+    def add_korean_font(self):
+        """
+        í•œê¸€ í°íŠ¸(NotoSansKR-Regular.ttf)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
+        í°íŠ¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ Arialë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.
+        """
+        # í°íŠ¸ íŒŒì¼ ê²½ë¡œ: í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ fonts/ ë””ë ‰í† ë¦¬
+        font_path = os.path.join(
+            os.path.dirname(os.path.abspath(__file__)),
+            "..",
+            "fonts",
+            "NotoSansKR-Regular.ttf",
+        )
+        font_path = os.path.normpath(font_path)  # ê²½ë¡œ ì •ê·œí™”
+
+        if os.path.exists(font_path):
+            try:
+                self.add_font("CustomFont", "", font_path, uni=True)
+                logging.info(f"âœ… í•œê¸€ í°íŠ¸ ë¡œë“œ ì„±ê³µ: {font_path}")
+            except Exception as e:
+                logging.error(
+                    f"âŒ í•œê¸€ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ({font_path}): {e}. Arialë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.",
+                    exc_info=True,
+                )
+                self.set_font("Arial", "", 12)
+        else:
+            logging.warning(
+                f"âŒ í•œê¸€ í°íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {font_path}. Arialë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤. fonts/NotoSansKR-Regular.ttfë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."
+            )
+            self.set_font("Arial", "", 12)  # í°íŠ¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ Arialë¡œ ëŒ€ì²´
+
+    def header(self):
+        self.set_font("CustomFont", "B", 16)
+        self.cell(0, 10, self.safe_text("ê¸°ì—… ì¬ë¬´ ë¶„ì„ ë³´ê³ ì„œ"), ln=True, align="C")
+        self.ln(5)
+
+    def footer(self):
+        self.set_y(-15)
+        self.set_font("CustomFont", "I", 8)
+        self.cell(0, 10, self.safe_text(f"í˜ì´ì§€ {self.page_no()}/{{nb}}"), 0, 0, "C")
+
+    def chapter_title(self, title):
+        self.set_font("CustomFont", "B", 12)
+        self.cell(0, 10, self.safe_text(title), ln=True)
+        self.ln(2)
+
+    def chapter_body(self, body):
+        self.set_font("CustomFont", "", 10)
+        self.multi_cell(0, 7, self.safe_text(body))
+        self.ln()
+
+    def add_image_from_bytes(self, img_bytes, w=180, h=0, x=None, y=None):
+        """BytesIO ê°ì²´ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
+        try:
+            self.image(img_bytes, x=x, y=y, w=w, h=h, type="PNG")
+            self.ln(5)
+        except Exception as e:
+            logging.error(f"ì´ë¯¸ì§€ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
+
+    def safe_text(self, text):
+        """FPDF í°íŠ¸ê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
+        if self.font_family == "Arial":  # Arial í°íŠ¸ ì‚¬ìš© ì‹œ ì´ëª¨í‹°ì½˜ ë“± ì œê±°
+            return "".join(ch for ch in text if ord(ch) <= 0xFFFF)
+        return text  # CustomFontëŠ” ìœ ë‹ˆì½”ë“œ ì§€ì›í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
+
+
+# ---------------------------------------------------
+# í—¬í¼ í•¨ìˆ˜: ê¸°ì—… ì½”ë“œ ì •ë³´ ì¡°íšŒ
+# ---------------------------------------------------
+def _get_corp_codes_info(corp_name: str) -> tuple[str, str]:
+    """
+    ê¸°ì—…ëª…ìœ¼ë¡œ corp_codeì™€ stock_codeë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
+    """
+    corp_codes_df = load_or_create_corp_code_list()
+    row = corp_codes_df[corp_codes_df["corp_name"] == corp_name]
+    if row.empty:
+        logging.error(f"ê¸°ì—…ëª… '{corp_name}'ì— í•´ë‹¹í•˜ëŠ” ê¸°ì—… ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
+        raise ValueError(f"ê¸°ì—…ëª… '{corp_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
+
+    row = row.iloc[0]
+    corp_code = row["corp_code"]
+    stock_code = (
+        str(row["stock_code"]).zfill(6) if pd.notna(row["stock_code"]) else None
+    )
+    return corp_code, stock_code
+
+
+# ---------------------------------------------------
+# í—¬í¼ í•¨ìˆ˜: ê¸°ì—… ê°œìš” ë° ë¡œê³  ì¡°íšŒ
+# ---------------------------------------------------
+def _fetch_company_overview_safe(corp_code: str) -> dict:
+    """DART APIì—ì„œ ê¸°ì—… ê°œìš”ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
+    url = "https://opendart.fss.or.kr/api/company.json"
+    params = {"crtfc_key": API_KEY, "corp_code": corp_code}
+    try:
+        res = requests.get(url, params=params, timeout=5)
+        res.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
+        data = res.json()
+        if data.get("status") == "000":
+            logging.info(f"âœ… ê¸°ì—… ê°œìš” ì¡°íšŒ ì„±ê³µ: {corp_code}")
+            return {
+                "ê¸°ì—…ëª…": data.get("corp_name"),
+                "ì˜ë¬¸ëª…": data.get("corp_name_eng"),
+                "ì—…ì¢…": data.get("industry"),
+                "ì„¤ë¦½ì¼": data.get("est_dt"),
+                "ëŒ€í‘œì": data.get("ceo_nm"),
+                "í™ˆí˜ì´ì§€": data.get("hm_url"),
+                "ì£¼ì†Œ": data.get("adres"),
+            }
+        else:
+            logging.warning(
+                f"DART ê¸°ì—… ê°œìš” ì¡°íšŒ ì‹¤íŒ¨ ({corp_code}): {data.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
+            )
+            return {}
+    except requests.exceptions.RequestException as e:
+        logging.error(f"DART ê¸°ì—… ê°œìš” API ìš”ì²­ ì˜¤ë¥˜ ({corp_code}): {e}", exc_info=True)
+        return {}
+    except Exception as e:
+        logging.error(
+            f"ê¸°ì—… ê°œìš” ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ ({corp_code}): {e}", exc_info=True
+        )
+        return {}
+
+
+def _fetch_company_logo_safe(stock_code: str) -> BytesIO | None:
+    """ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ê¸°ì—… ë¡œê³ ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
+    if not stock_code:
+        return None
+    try:
+        logo_url = (
+            f"https://ssl.pstatic.net/imgfinance/chart/item/200x200/{stock_code}.png"
+        )
+        res = requests.get(logo_url, timeout=5)
+        res.raise_for_status()
+        if res.status_code == 200 and res.content:
+            logging.info(f"âœ… ê¸°ì—… ë¡œê³  ì¡°íšŒ ì„±ê³µ: {stock_code}")
+            return BytesIO(res.content)
+        else:
+            logging.warning(
+                f"ê¸°ì—… ë¡œê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {stock_code}"
+            )
+            return None
+    except requests.exceptions.RequestException as e:
+        logging.warning(f"ê¸°ì—… ë¡œê³  ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({stock_code}): {e}")
+        return None
+    except Exception as e:
+        logging.error(
+            f"ê¸°ì—… ë¡œê³  ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ ({stock_code}): {e}",
+            exc_info=True,
+        )
+        return None
+
+
+# ---------------------------------------------------
+# í—¬í¼ í•¨ìˆ˜: Plotly ì°¨íŠ¸ ìƒì„±
+# ---------------------------------------------------
+def _create_price_chart(price_df: pd.DataFrame, corp_name: str) -> BytesIO | None:
+    """
+    ì£¼ê°€ ì¶”ì´ ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ê³  BytesIO ê°ì²´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
+    """
+    if (
+        price_df.empty
+        or "date" not in price_df.columns
+        or "close" not in price_df.columns
+    ):
+        logging.warning(
+            f"ì£¼ê°€ ë°ì´í„°ê°€ ë¶ˆì™„ì „í•˜ì—¬ ì£¼ê°€ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {corp_name}"
+        )
+        return None
+
+    # price_dfì˜ ì»¬ëŸ¼ëª…ì„ ì˜ë¬¸í™” (data_fetchì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ ë‹¤ì‹œ í™•ì¸)
+    if "ë‚ ì§œ" in price_df.columns:
+        price_df.rename(
+            columns={
+                "ë‚ ì§œ": "date",
+                "ì¢…ê°€": "close",
+                "ì „ì¼ë¹„": "diff",
+                "ì‹œê°€": "open",
+                "ê³ ê°€": "high",
+                "ì €ê°€": "low",
+                "ê±°ë˜ëŸ‰": "volume",
+            },
+            inplace=True,
+        )
+
+    fig_price = px.line(
+        price_df,
+        x="date",
+        y="close",
+        title=f"{corp_name} ì£¼ê°€ ì¶”ì´",
+        labels={"date": "ë‚ ì§œ", "close": "ì¢…ê°€"},
+        template="plotly_white",  # ê¹”ë”í•œ ë°°ê²½
+    )
+    fig_price.update_layout(title_x=0.5)  # ì œëª© ì¤‘ì•™ ì •ë ¬
+    try:
+        img_bytes = BytesIO()
+        fig_price.write_image(img_bytes, format="png", scale=2)  # ê³ í•´ìƒë„ PNG
+        img_bytes.seek(0)
+        logging.info(f"âœ… ì£¼ê°€ ì°¨íŠ¸ ìƒì„± ì„±ê³µ: {corp_name}")
+        return img_bytes
+    except Exception as e:
+        logging.error(f"ì£¼ê°€ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨ ({corp_name}): {e}", exc_info=True)
+        return None
+
+
+def _create_ratio_chart(health: dict, corp_name: str, year: int) -> BytesIO | None:
+    """
+    ì£¼ìš” ì¬ë¬´ë¹„ìœ¨ ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ê³  BytesIO ê°ì²´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
+    """
+    if not health or all(pd.isna(v) for v in health.values()):
+        logging.warning(
+            f"ì¬ë¬´ ê±´ì „ì„± ë°ì´í„°ê°€ ì—†ì–´ ì¬ë¬´ë¹„ìœ¨ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {corp_name}"
+        )
+        return None
+
+    # ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„
+    ratio_names = [
+        "ROE",
+        "ë¶€ì±„ë¹„ìœ¨",
+        "ìœ ë™ë¹„ìœ¨",
+        "ì˜ì—…ì´ìµë¥ ",
+        "ì´ìë³´ìƒë°°ìœ¨",
+        "Z-score",
+    ]
+    ratio_values = [
+        health.get("roe", np.nan),
+        health.get("debt_ratio", np.nan),
+        health.get("current_ratio", np.nan),
+        health.get("op_margin", np.nan),
+        health.get("interest_coverage", np.nan),
+        health.get("z_score", np.nan),
+    ]
+
+    # NaN ê°’ì€ ì°¨íŠ¸ì— í‘œì‹œë˜ì§€ ì•Šë„ë¡ í•„í„°ë§
+    filtered_ratios = [
+        (name, value)
+        for name, value in zip(ratio_names, ratio_values)
+        if not pd.isna(value)
+    ]
+    if not filtered_ratios:
+        logging.warning(
+            f"ìœ íš¨í•œ ì¬ë¬´ë¹„ìœ¨ ë°ì´í„°ê°€ ì—†ì–´ ì¬ë¬´ë¹„ìœ¨ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {corp_name}"
+        )
+        return None
+
+    chart_df = pd.DataFrame(filtered_ratios, columns=["Ratio", "Value"])
+
+    fig_ratio = px.bar(
+        chart_df,
+        x="Ratio",
+        y="Value",
+        title=f"{corp_name} ì£¼ìš” ì¬ë¬´ë¹„ìœ¨ ({year})",
+        labels={"Ratio": "ì¬ë¬´ë¹„ìœ¨", "Value": "ê°’"},
+        template="plotly_white",
+        color_discrete_sequence=px.colors.qualitative.Pastel,  # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ë³€ê²½
+    )
+    fig_ratio.update_layout(title_x=0.5)
+    try:
+        img_bytes = BytesIO()
+        fig_ratio.write_image(img_bytes, format="png", scale=2)
+        img_bytes.seek(0)
+        logging.info(f"âœ… ì¬ë¬´ë¹„ìœ¨ ì°¨íŠ¸ ìƒì„± ì„±ê³µ: {corp_name}")
+        return img_bytes
+    except Exception as e:
+        logging.error(
+            f"ì¬ë¬´ë¹„ìœ¨ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨ ({corp_name}): {e}", exc_info=True
+        )
+        return None
+
+
+# ---------------------------------------------------
+# Excel ë³´ê³ ì„œ ì €ì¥ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
+# ---------------------------------------------------
+def save_excel_report(
+    df_results: pd.DataFrame,
+    df_industry_avg: pd.DataFrame,
+    filename: str = "financial_report.xlsx",
+) -> str:
+    """
+    ë¶„ì„ ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
+    """
+    file_path = os.path.join("data/reports", filename)
+    try:
+        with pd.ExcelWriter(file_path, engine="xlsxwriter") as writer:
+            df_results.to_excel(writer, sheet_name="ê¸°ì—…ë³„ ë¶„ì„", index=False)
+            df_industry_avg.to_excel(writer, sheet_name="ì—…ì¢…ë³„ í‰ê· ")
+        logging.info(f"âœ… Excel ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {file_path}")
+        return file_path
+    except Exception as e:
+        logging.error(f"âŒ Excel ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨ ({file_path}): {e}", exc_info=True)
+        return ""
+
+
+# ---------------------------------------------------
+# í†µí•© íˆ¬ì ë¦¬í¬íŠ¸ ìƒì„± (PDF)
+# ---------------------------------------------------
+def generate_investment_report(
+    corp_name: str, year: int, health: dict, fs_df: pd.DataFrame, price_df: pd.DataFrame
+) -> BytesIO | None:
+    """
+    ì¦ê¶Œì‚¬ ë¦¬ì„œì¹˜ì„¼í„° ìŠ¤íƒ€ì¼ì˜ í†µí•© íˆ¬ì ë¦¬í¬íŠ¸(PDF)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
+
+    Args:
+        corp_name (str): ê¸°ì—…ëª….
+        year (int): ë¶„ì„ ì—°ë„.
+        health (dict): calculate_financial_health í•¨ìˆ˜ì—ì„œ ë°˜í™˜ëœ ì¬ë¬´ ê±´ì „ì„± ì§€í‘œ.
+        fs_df (pd.DataFrame): ì¬ë¬´ì œí‘œ ë°ì´í„° DataFrame.
+        price_df (pd.DataFrame): ì£¼ê°€ ë°ì´í„° DataFrame.
+
+    Returns:
+        BytesIO | None: ìƒì„±ëœ PDF íŒŒì¼ì˜ BytesIO ê°ì²´. ì‹¤íŒ¨ ì‹œ None.
+    """
+    try:
+        # 1. ê¸°ì—… ì½”ë“œ ë° ì£¼ì‹ ì½”ë“œ ì¡°íšŒ
+        corp_code, stock_code = _get_corp_codes_info(corp_name)
+        if not corp_code:
+            return None
+
+        # 2. ê¸°ì—… ê°œìš” ë° ë¡œê³  ì¡°íšŒ
+        overview = _fetch_company_overview_safe(corp_code)
+        logo_img_bytes = _fetch_company_logo_safe(stock_code)
+
+        # 3. ì¬ë¬´ì œí‘œ ìš”ì•½
+        fs_summary = extract_fs_summary(fs_df)
+
+        # 4. ì°¨íŠ¸ ìƒì„± (BytesIOë¡œ ì§ì ‘ ë°›ìŒ)
+        price_chart_bytes = _create_price_chart(price_df, corp_name)
+        ratio_chart_bytes = _create_ratio_chart(health, corp_name, year)
+
+        # 5. PDF ìƒì„± ì‹œì‘
+        pdf = PDFReport()
+        pdf.add_page()
+        pdf.set_left_margin(20)
+        pdf.set_right_margin(20)
+
+        # ì œëª© ì„¹ì…˜
+        pdf.set_font("CustomFont", "B", 24)
+        pdf.cell(
+            0, 15, pdf.safe_text(f"{corp_name} {year} íˆ¬ì ë¦¬í¬íŠ¸"), ln=True, align="C"
+        )
+        pdf.ln(5)
+
+        # ë¡œê³  ì¶”ê°€ (ìˆë‹¤ë©´)
+        if logo_img_bytes:
+            # ë¡œê³ ë¥¼ ì˜¤ë¥¸ìª½ ìƒë‹¨ì— ë°°ì¹˜
+            pdf.image(logo_img_bytes, x=pdf.w - 40, y=10, w=30)
+            logo_img_bytes.seek(0)  # ì¬ì‚¬ìš©ì„ ìœ„í•´ í¬ì¸í„° ì´ˆê¸°í™”
+
+        # êµ¬ë¶„ì„ 
+        pdf.set_draw_color(0, 0, 0)
+        pdf.set_line_width(0.5)
+        pdf.line(20, pdf.get_y(), pdf.w - 20, pdf.get_y())
+        pdf.ln(10)
+
+        # ì¢…í•© ì ìˆ˜ ë° ë“±ê¸‰
+        pdf.set_font("CustomFont", "", 14)
+        total_score_str = (
+            f"{health['total_score']:.2f}"
+            if not pd.isna(health.get("total_score"))
+            else "N/A"
+        )
+        grade_str = health.get("grade", "N/A")
+        pdf.cell(
+            0,
+            10,
+            pdf.safe_text(f"âœ¨ ì¢…í•© ì ìˆ˜: {total_score_str} | ë“±ê¸‰: {grade_str}"),
+            ln=True,
+        )
+        pdf.ln(5)
+
+        # ê¸°ì—… ê°œìš”
+        if overview:
+            pdf.set_font("CustomFont", "B", 14)
+            pdf.cell(0, 10, pdf.safe_text("ğŸ“„ ê¸°ì—… ê°œìš”"), ln=True)
+            pdf.set_font("CustomFont", "", 11)
+            for k, v in overview.items():
+                if v:
+                    pdf.cell(0, 7, pdf.safe_text(f"{k}: {v}"), ln=True)
+            pdf.ln(5)
+
+        # ì¬ë¬´ì œí‘œ ìš”ì•½
+        if fs_summary:
+            pdf.set_font("CustomFont", "B", 14)
+            pdf.cell(0, 10, pdf.safe_text("ğŸ“Š ì¬ë¬´ì œí‘œ ìš”ì•½"), ln=True)
+            pdf.set_font("CustomFont", "", 11)
+            for k, v in fs_summary.items():
+                if not pd.isna(v):
+                    pdf.cell(
+                        0, 7, pdf.safe_text(f"{k}: {v:,.0f} ì›"), ln=True
+                    )  # ì²œë‹¨ìœ„ ì½¤ë§ˆ, ì› í‘œì‹œ
+            pdf.ln(5)
+
+        # ì¬ë¬´ë¹„ìœ¨ ì°¨íŠ¸
+        if ratio_chart_bytes:
+            pdf.add_page()  # ìƒˆ í˜ì´ì§€ì— ì°¨íŠ¸ ì¶”ê°€
+            pdf.set_font("CustomFont", "B", 14)
+            pdf.cell(0, 10, pdf.safe_text("ğŸ“ˆ ì£¼ìš” ì¬ë¬´ë¹„ìœ¨ ë¶„ì„"), ln=True)
+            pdf.ln(5)
+            pdf.add_image_from_bytes(ratio_chart_bytes, w=180)
+            ratio_chart_bytes.seek(0)  # ì¬ì‚¬ìš©ì„ ìœ„í•´ í¬ì¸í„° ì´ˆê¸°í™”
+            pdf.ln(5)
+
+        # ì£¼ê°€ ì°¨íŠ¸
+        if price_chart_bytes:
+            if (
+                pdf.get_y() > pdf.h - 80
+            ):  # í˜„ì¬ í˜ì´ì§€ í•˜ë‹¨ì— ê³µê°„ì´ ë¶€ì¡±í•˜ë©´ ìƒˆ í˜ì´ì§€ ì‹œì‘
+                pdf.add_page()
+            pdf.set_font("CustomFont", "B", 14)
+            pdf.cell(0, 10, pdf.safe_text("ğŸ’¹ ì£¼ê°€ ì¶”ì´ ë¶„ì„"), ln=True)
+            pdf.ln(5)
+            pdf.add_image_from_bytes(price_chart_bytes, w=180)
+            price_chart_bytes.seek(0)  # ì¬ì‚¬ìš©ì„ ìœ„í•´ í¬ì¸í„° ì´ˆê¸°í™”
+            pdf.ln(5)
+
+        # PDFë¥¼ BytesIO ê°ì²´ë¡œ ì €ì¥
+        pdf_output = BytesIO()
+        pdf.output(pdf_output)
+        pdf_output.seek(0)  # ìŠ¤íŠ¸ë¦¼ì˜ ì‹œì‘ìœ¼ë¡œ í¬ì¸í„° ì´ë™
+
+        logging.info(f"âœ… '{corp_name}' {year} íˆ¬ì ë¦¬í¬íŠ¸ PDF ìƒì„± ì™„ë£Œ.")
+        return pdf_output
+
+    except ValueError as e:
+        logging.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜ (ë°ì´í„° ë¬¸ì œ): {e}")
+        return None
+    except Exception as e:
+        logging.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
+        return None
+
+
+# ê¸°ì¡´ generate_investment_report_full ë° create_visual_pdfëŠ” ì œê±°ë¨
+# generate_investment_report í•¨ìˆ˜ê°€ ëª¨ë“  ê¸°ëŠ¥ì„ í†µí•©í•¨.
diff --git a/packages/core/build/lib/core/_legacy_visualization.py b/packages/core/build/lib/core/_legacy_visualization.py
new file mode 100644
index 0000000..fefb0d0
--- /dev/null
+++ b/packages/core/build/lib/core/_legacy_visualization.py
@@ -0,0 +1,376 @@
+import plotly.express as px
+import plotly.graph_objects as go
+import pandas as pd
+import numpy as np
+import logging
+
+# ë¡œê¹… ì„¤ì • (ë‹¤ë¥¸ ëª¨ë“ˆê³¼ ì¼ê´€ì„± ìœ ì§€)
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
+)
+
+
+# ---------------------------------------------------
+# í—¬í¼ í•¨ìˆ˜: ê³µí†µ ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸
+# ---------------------------------------------------
+def _update_common_layout(fig: go.Figure, title: str, dark_mode: bool = False):
+    """
+    Plotly ì°¨íŠ¸ì˜ ê³µí†µ ë ˆì´ì•„ì›ƒì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
+    Args:
+        fig (go.Figure): Plotly Figure ê°ì²´.
+        title (str): ì°¨íŠ¸ ì œëª©.
+        dark_mode (bool): ë‹¤í¬ ëª¨ë“œ ì ìš© ì—¬ë¶€.
+    """
+    template = "plotly_dark" if dark_mode else "plotly_white"
+    fig.update_layout(
+        title_text=title,
+        title_x=0.5,  # ì œëª© ì¤‘ì•™ ì •ë ¬
+        template=template,
+        hovermode="x unified",  # ë§ˆìš°ìŠ¤ ì˜¤ë²„ ì‹œ ì •ë³´ í‘œì‹œ ë°©ì‹
+        font=dict(
+            family="Arial, sans-serif", size=12, color="white" if dark_mode else "black"
+        ),
+        paper_bgcolor=(
+            "rgba(0,0,0,0)" if dark_mode else "rgba(0,0,0,0)"
+        ),  # ë°°ê²½ íˆ¬ëª…í•˜ê²Œ
+        plot_bgcolor=(
+            "rgba(0,0,0,0)" if dark_mode else "rgba(0,0,0,0)"
+        ),  # í”Œë¡¯ ì˜ì—­ ë°°ê²½ íˆ¬ëª…í•˜ê²Œ
+    )
+    # ì¶• ìƒ‰ìƒ ì„¤ì •
+    fig.update_xaxes(
+        showgrid=True,
+        gridwidth=1,
+        gridcolor="rgba(150,150,150,0.2)" if dark_mode else "rgba(200,200,200,0.5)",
+        zeroline=False,
+        linecolor="rgba(150,150,150,0.5)" if dark_mode else "rgba(100,100,100,0.5)",
+        tickfont=dict(color="white" if dark_mode else "black"),
+        title_font=dict(color="white" if dark_mode else "black"),
+    )
+    fig.update_yaxes(
+        showgrid=True,
+        gridwidth=1,
+        gridcolor="rgba(150,150,150,0.2)" if dark_mode else "rgba(200,200,200,0.5)",
+        zeroline=False,
+        linecolor="rgba(150,150,150,0.5)" if dark_mode else "rgba(100,100,100,0.5)",
+        tickfont=dict(color="white" if dark_mode else "black"),
+        title_font=dict(color="white" if dark_mode else "black"),
+    )
+
+
+# ---------------------------------------------------
+# ì¬ë¬´ ë¹„ìœ¨ ë§‰ëŒ€ê·¸ë˜í”„
+# ---------------------------------------------------
+def plot_ratios(
+    health_dict: dict, title: str = "ì£¼ìš” ì¬ë¬´ ë¹„ìœ¨", dark_mode: bool = False
+) -> go.Figure:
+    """
+    ì¬ë¬´ ê±´ì „ì„± ì§€í‘œë¥¼ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
+    Args:
+        health_dict (dict): ì¬ë¬´ ê±´ì „ì„± ì§€í‘œ ë”•ì…”ë„ˆë¦¬ (calculate_financial_health ê²°ê³¼).
+        title (str): ì°¨íŠ¸ ì œëª©.
+        dark_mode (bool): ë‹¤í¬ ëª¨ë“œ ì ìš© ì—¬ë¶€.
+    Returns:
+        go.Figure: Plotly Figure ê°ì²´.
+    """
+    if not health_dict or all(pd.isna(v) for v in health_dict.values()):
+        logging.warning(
+            "ì¬ë¬´ ê±´ì „ì„± ë°ì´í„°ê°€ ë¹„ì–´ìˆì–´ ì¬ë¬´ ë¹„ìœ¨ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
+        )
+        fig = go.Figure()
+        _update_common_layout(fig, f"{title} (ë°ì´í„° ì—†ìŒ)", dark_mode)
+        return fig
+
+    # total_scoreì™€ grade ì œì™¸
+    df_ratios = pd.DataFrame(health_dict.items(), columns=["ì§€í‘œ", "ê°’"])
+    df_ratios = df_ratios[~df_ratios["ì§€í‘œ"].isin(["total_score", "grade"])]
+
+    # NaN ê°’ ì œê±° (ì°¨íŠ¸ì— í‘œì‹œë˜ì§€ ì•Šë„ë¡)
+    df_ratios = df_ratios.dropna(subset=["ê°’"])
+
+    if df_ratios.empty:
+        logging.warning("ìœ íš¨í•œ ì¬ë¬´ ë¹„ìœ¨ ë°ì´í„°ê°€ ì—†ì–´ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
+        fig = go.Figure()
+        _update_common_layout(fig, f"{title} (ë°ì´í„° ì—†ìŒ)", dark_mode)
+        return fig
+
+    fig = px.bar(
+        df_ratios,
+        x="ì§€í‘œ",
+        y="ê°’",
+        text="ê°’",
+        color="ì§€í‘œ",
+        color_discrete_sequence=px.colors.qualitative.Pastel,  # ë¶€ë“œëŸ¬ìš´ ìƒ‰ìƒ íŒ”ë ˆíŠ¸
+    )
+    fig.update_traces(
+        texttemplate="%{text:.2f}",
+        textposition="outside",
+        marker_line_color="rgb(8,48,107)",
+        marker_line_width=1.5,
+        opacity=0.8,
+    )
+    fig.update_layout(yaxis_title="ê°’", xaxis_title="ì§€í‘œ", showlegend=False)
+
+    _update_common_layout(fig, title, dark_mode)
+    logging.info(f"âœ… ì¬ë¬´ ë¹„ìœ¨ ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {title}")
+    return fig
+
+
+# ---------------------------------------------------
+# ì£¼ê°€ ë°ì´í„° ì‹œê°í™” (ë¼ì¸ ì°¨íŠ¸ ë˜ëŠ” ìº”ë“¤ìŠ¤í‹± ì°¨íŠ¸)
+# ---------------------------------------------------
+def plot_price(
+    price_df: pd.DataFrame,
+    corp_name: str,
+    chart_type: str = "line",
+    dark_mode: bool = False,
+) -> go.Figure:
+    """
+    ì£¼ê°€ ë°ì´í„°ë¥¼ ë¼ì¸ ì°¨íŠ¸ ë˜ëŠ” ìº”ë“¤ìŠ¤í‹± ì°¨íŠ¸ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
+    Args:
+        price_df (pd.DataFrame): ì£¼ê°€ ë°ì´í„° DataFrame (date, open, high, low, close, volume ì»¬ëŸ¼ í¬í•¨).
+        corp_name (str): ê¸°ì—…ëª….
+        chart_type (str): "line" (ë¼ì¸ ì°¨íŠ¸) ë˜ëŠ” "candlestick" (ìº”ë“¤ìŠ¤í‹± ì°¨íŠ¸). ê¸°ë³¸ê°’ "line".
+        dark_mode (bool): ë‹¤í¬ ëª¨ë“œ ì ìš© ì—¬ë¶€.
+    Returns:
+        go.Figure: Plotly Figure ê°ì²´.
+    """
+    if price_df.empty or "date" not in price_df.columns:
+        logging.warning(
+            f"{corp_name} ì£¼ê°€ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ 'date' ì»¬ëŸ¼ì´ ì—†ì–´ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
+        )
+        fig = go.Figure()
+        _update_common_layout(fig, f"{corp_name} ì£¼ê°€ ì¶”ì´ (ë°ì´í„° ì—†ìŒ)", dark_mode)
+        return fig
+
+    # ì»¬ëŸ¼ëª… ì˜ë¬¸í™” (data_fetchì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ ë‹¤ì‹œ í™•ì¸)
+    if "ë‚ ì§œ" in price_df.columns:
+        price_df.rename(
+            columns={
+                "ë‚ ì§œ": "date",
+                "ì¢…ê°€": "close",
+                "ì „ì¼ë¹„": "diff",
+                "ì‹œê°€": "open",
+                "ê³ ê°€": "high",
+                "ì €ê°€": "low",
+                "ê±°ë˜ëŸ‰": "volume",
+            },
+            inplace=True,
+        )
+
+    # 'date' ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
+    price_df["date"] = pd.to_datetime(price_df["date"])
+    price_df = price_df.sort_values("date")  # ë‚ ì§œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
+
+    fig = go.Figure()
+    chart_title = f"{corp_name} ì£¼ê°€ ì¶”ì´"
+
+    if chart_type == "candlestick":
+        # ìº”ë“¤ìŠ¤í‹± ì°¨íŠ¸ ìƒì„±ì— í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
+        required_cols = ["open", "high", "low", "close"]
+        if not all(col in price_df.columns for col in required_cols):
+            logging.warning(
+                f"ìº”ë“¤ìŠ¤í‹± ì°¨íŠ¸ ìƒì„±ì— í•„ìš”í•œ ì»¬ëŸ¼({required_cols})ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë¼ì¸ ì°¨íŠ¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."
+            )
+            chart_type = "line"  # í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¼ì¸ ì°¨íŠ¸ë¡œ ëŒ€ì²´
+
+        if chart_type == "candlestick":
+            fig.add_trace(
+                go.Candlestick(
+                    x=price_df["date"],
+                    open=price_df["open"],
+                    high=price_df["high"],
+                    low=price_df["low"],
+                    close=price_df["close"],
+                    name="ì£¼ê°€",
+                )
+            )
+            chart_title = f"{corp_name} ì£¼ê°€ ìº”ë“¤ìŠ¤í‹± ì°¨íŠ¸"
+            fig.update_layout(
+                xaxis_rangeslider_visible=False
+            )  # ìº”ë“¤ìŠ¤í‹± ê¸°ë³¸ ìŠ¬ë¼ì´ë” ìˆ¨ê¹€
+
+    if chart_type == "line":
+        fig.add_trace(
+            go.Scatter(
+                x=price_df["date"],
+                y=price_df["close"],
+                mode="lines",
+                name="ì¢…ê°€",
+                line=dict(color="royalblue", width=2),
+            )
+        )
+        chart_title = f"{corp_name} ì£¼ê°€ ë¼ì¸ ì°¨íŠ¸"
+
+    # ë²”ìœ„ ìŠ¬ë¼ì´ë” ì¶”ê°€
+    fig.update_layout(
+        xaxis_rangeslider_visible=True,
+        xaxis_rangeselector=dict(
+            buttons=list(
+                [
+                    dict(count=1, label="1m", step="month", stepmode="backward"),
+                    dict(count=6, label="6m", step="month", stepmode="backward"),
+                    dict(count=1, label="1y", step="year", stepmode="backward"),
+                    dict(step="all"),
+                ]
+            )
+        ),
+    )
+
+    _update_common_layout(fig, chart_title, dark_mode)
+    logging.info(f"âœ… ì£¼ê°€ ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {corp_name} ({chart_type})")
+    return fig
+
+
+# ---------------------------------------------------
+# ì—…ì¢…ë³„ í‰ê·  ì§€í‘œ ë§‰ëŒ€ê·¸ë˜í”„
+# ---------------------------------------------------
+def plot_industry_avg(
+    industry_avg_df: pd.DataFrame,
+    title: str = "ì—…ì¢…ë³„ í‰ê·  ì´ì ",
+    dark_mode: bool = False,
+) -> go.Figure:
+    """
+    ì—…ì¢…ë³„ í‰ê·  ì´ì ì„ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
+    Args:
+        industry_avg_df (pd.DataFrame): ì—…ì¢…ë³„ í‰ê·  ë°ì´í„° DataFrame.
+        title (str): ì°¨íŠ¸ ì œëª©.
+        dark_mode (bool): ë‹¤í¬ ëª¨ë“œ ì ìš© ì—¬ë¶€.
+    Returns:
+        go.Figure: Plotly Figure ê°ì²´.
+    """
+    if (
+        industry_avg_df.empty
+        or "ì—…ì¢…" not in industry_avg_df.columns
+        or "total_score" not in industry_avg_df.columns
+    ):
+        logging.warning(
+            "ì—…ì¢…ë³„ í‰ê·  ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ì–´ ì—…ì¢… í‰ê·  ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
+        )
+        fig = go.Figure()
+        _update_common_layout(fig, f"{title} (ë°ì´í„° ì—†ìŒ)", dark_mode)
+        return fig
+
+    # 'ì´ì ' ëŒ€ì‹  'total_score' ì»¬ëŸ¼ ì‚¬ìš© (analysis.pyì˜ calculate_financial_health ê²°ê³¼ì™€ ì¼ê´€ì„± ìœ ì§€)
+    df = industry_avg_df.reset_index()
+
+    fig = px.bar(
+        df,
+        x="ì—…ì¢…",
+        y="total_score",  # 'ì´ì ' ëŒ€ì‹  'total_score' ì‚¬ìš©
+        text="total_score",
+        color="ì—…ì¢…",
+        color_discrete_sequence=px.colors.qualitative.Set3,
+    )
+    fig.update_traces(
+        texttemplate="%{text:.2f}",
+        textposition="outside",
+        marker_line_color="rgb(8,48,107)",
+        marker_line_width=1.5,
+        opacity=0.8,
+    )
+    fig.update_layout(yaxis_title="ì´ì ", xaxis_title="ì—…ì¢…", showlegend=False)
+
+    _update_common_layout(fig, title, dark_mode)
+    logging.info(f"âœ… ì—…ì¢…ë³„ í‰ê·  ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {title}")
+    return fig
+
+
+# ---------------------------------------------------
+# ê°œë³„ ê¸°ì—… vs ì—…ì¢… í‰ê·  ì§€í‘œ ë¹„êµ ë§‰ëŒ€ê·¸ë˜í”„ (ìƒˆë¡œ ì¶”ê°€)
+# ---------------------------------------------------
+def plot_company_vs_industry_avg(
+    company_data: pd.Series,
+    industry_avg_df: pd.DataFrame,
+    metric_name: str,
+    company_name: str,
+    dark_mode: bool = False,
+) -> go.Figure:
+    """
+    ê°œë³„ ê¸°ì—…ì˜ íŠ¹ì • ì§€í‘œë¥¼ í•´ë‹¹ ê¸°ì—…ì˜ ì—…ì¢… í‰ê· ê³¼ ë¹„êµí•˜ëŠ” ë§‰ëŒ€ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
+    Args:
+        company_data (pd.Series): ì„ íƒëœ ê¸°ì—…ì˜ ë¶„ì„ ê²°ê³¼ (ì˜ˆ: multi_results_dfì˜ í•œ í–‰).
+        industry_avg_df (pd.DataFrame): ì—…ì¢…ë³„ í‰ê·  ë°ì´í„° DataFrame.
+        metric_name (str): ë¹„êµí•  ì§€í‘œì˜ ì´ë¦„ (ì˜ˆ: 'ì´ì ', 'ROE', 'PER').
+        company_name (str): ë¹„êµí•  ê¸°ì—…ì˜ ì´ë¦„.
+        dark_mode (bool): ë‹¤í¬ ëª¨ë“œ ì ìš© ì—¬ë¶€.
+    Returns:
+        go.Figure: Plotly Figure ê°ì²´.
+    """
+    if (
+        company_data.empty
+        or metric_name not in company_data.index
+        or pd.isna(company_data[metric_name])
+    ):
+        logging.warning(
+            f"ê¸°ì—… '{company_name}'ì˜ '{metric_name}' ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ë¹„êµ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
+        )
+        fig = go.Figure()
+        _update_common_layout(
+            fig,
+            f"'{company_name}' vs ì—…ì¢… í‰ê·  ({metric_name}) (ë°ì´í„° ì—†ìŒ)",
+            dark_mode,
+        )
+        return fig
+
+    company_industry = company_data.get("ì—…ì¢…", "ì•Œ ìˆ˜ ì—†ìŒ")
+
+    # í•´ë‹¹ ì—…ì¢…ì˜ í‰ê· ê°’ ì°¾ê¸°
+    industry_avg_value = np.nan
+    if "ì—…ì¢…" in industry_avg_df.columns and metric_name in industry_avg_df.columns:
+        avg_row = industry_avg_df[industry_avg_df["ì—…ì¢…"] == company_industry]
+        if not avg_row.empty:
+            industry_avg_value = avg_row.iloc[0][metric_name]
+
+    if pd.isna(industry_avg_value):
+        logging.warning(
+            f"ì—…ì¢… '{company_industry}'ì˜ '{metric_name}' í‰ê·  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¹„êµ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
+        )
+        fig = go.Figure()
+        _update_common_layout(
+            fig,
+            f"'{company_name}' vs ì—…ì¢… í‰ê·  ({metric_name}) (ì—…ì¢… í‰ê·  ë°ì´í„° ì—†ìŒ)",
+            dark_mode,
+        )
+        return fig
+
+    # ì°¨íŠ¸ ë°ì´í„° ìƒì„±
+    data = {
+        "Category": [company_name, f"{company_industry} í‰ê· "],
+        "Value": [company_data[metric_name], industry_avg_value],
+    }
+    df_plot = pd.DataFrame(data)
+
+    fig = px.bar(
+        df_plot,
+        x="Category",
+        y="Value",
+        text="Value",
+        title=f"'{company_name}' ({company_industry}) vs ì—…ì¢… í‰ê· : {metric_name}",
+        labels={"Category": "êµ¬ë¶„", "Value": metric_name},
+        color="Category",
+        color_discrete_map={
+            company_name: px.colors.qualitative.Pastel[0],  # ê¸°ì—… ìƒ‰ìƒ
+            f"{company_industry} í‰ê· ": px.colors.qualitative.Pastel[
+                1
+            ],  # ì—…ì¢… í‰ê·  ìƒ‰ìƒ
+        },
+    )
+    fig.update_traces(
+        texttemplate="%{text:.2f}",
+        textposition="outside",
+        marker_line_color="rgb(8,48,107)",
+        marker_line_width=1.5,
+        opacity=0.8,
+    )
+    fig.update_layout(showlegend=False)  # ë²”ë¡€ ìˆ¨ê¸°ê¸°
+
+    _update_common_layout(
+        fig,
+        f"'{company_name}' ({company_industry}) vs ì—…ì¢… í‰ê· : {metric_name}",
+        dark_mode,
+    )
+    logging.info(
+        f"âœ… ê¸°ì—… vs ì—…ì¢… í‰ê·  ë¹„êµ ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {company_name} - {metric_name}"
+    )
+    return fig
diff --git a/packages/core/build/lib/core/clients/__init__.py b/packages/core/build/lib/core/clients/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/packages/core/build/lib/core/clients/dart.py b/packages/core/build/lib/core/clients/dart.py
new file mode 100644
index 0000000..dfec3f5
--- /dev/null
+++ b/packages/core/build/lib/core/clients/dart.py
@@ -0,0 +1,23 @@
+import httpx
+
+
+class DARTClient:
+    def __init__(self, api_key: str, *, timeout: float = 10.0):
+        self._client = httpx.AsyncClient(timeout=timeout)
+        self.api_key = api_key
+
+    async def single_fs(
+        self, corp_code: str, year: int, reprt_code: str, fs_div: str
+    ) -> dict:
+        r = await self._client.get(
+            "https://opendart.fss.or.kr/api/fnlttSinglAcntAll.json",
+            params={
+                "crtfc_key": self.api_key,
+                "corp_code": corp_code,
+                "bsns_year": str(year),
+                "reprt_code": reprt_code,
+                "fs_div": fs_div,
+            },
+        )
+        r.raise_for_status()
+        return r.json()
diff --git a/packages/core/build/lib/core/clients/kis.py b/packages/core/build/lib/core/clients/kis.py
new file mode 100644
index 0000000..79f5e37
--- /dev/null
+++ b/packages/core/build/lib/core/clients/kis.py
@@ -0,0 +1,83 @@
+from __future__ import annotations
+import asyncio
+from datetime import datetime, timedelta
+from typing import Any, Dict, Optional
+import httpx
+
+
+class KISClient:
+    def __init__(
+        self,
+        base_url: str,
+        app_key: str,
+        app_secret: str,
+        *,
+        timeout: float = 10.0,
+        oauth_path: str = "/oauth2/tokenP",
+    ):
+        self.base_url = base_url.rstrip("/")
+        self.app_key = app_key
+        self.app_secret = app_secret
+        self.oauth_path = oauth_path
+        self._client = httpx.AsyncClient(timeout=timeout)
+        self._token: Optional[str] = None
+        self._expires_at: Optional[datetime] = None
+        self._lock = asyncio.Lock()
+
+    async def _ensure_token(self) -> str:
+        async with self._lock:
+            if self._token and self._expires_at and datetime.now() < self._expires_at:
+                return self._token
+
+            # tiny retry to dodge transient 403s
+            last_exc = None
+            for _ in range(2):
+                r = await self._client.post(
+                    f"{self.base_url}{self.oauth_path}",
+                    json={
+                        "grant_type": "client_credentials",
+                        "appkey": self.app_key,
+                        "appsecret": self.app_secret,
+                    },
+                )
+                try:
+                    r.raise_for_status()
+                    data = r.json()
+                    self._token = data["access_token"]
+                    ttl = int(data.get("expires_in", 86400)) - 300
+                    self._expires_at = datetime.now() + timedelta(seconds=max(ttl, 600))
+                    return self._token
+                except httpx.HTTPStatusError as e:
+                    last_exc = e
+                    await asyncio.sleep(0.2)  # brief backoff
+
+            # surface helpful message
+            detail = getattr(last_exc.response, "text", "")[:200].replace("\n", " ")
+            raise httpx.HTTPStatusError(
+                f"KIS token failed ({last_exc.response.status_code}). Hint: {detail}",
+                request=last_exc.request,
+                response=last_exc.response,
+            )
+
+    async def get(self, path: str, *, tr_id: str, params: Dict[str, Any]) -> dict:
+        token = await self._ensure_token()
+        headers = {
+            "Authorization": f"Bearer {token}",
+            "appkey": self.app_key,
+            "appsecret": self.app_secret,
+            "tr_id": tr_id,
+            "custtype": "P",
+        }
+        r = await self._client.get(
+            f"{self.base_url}{path}", params=params, headers=headers
+        )
+        r.raise_for_status()
+        data = r.json()
+        if data.get("rt_cd") != "0":
+            raise httpx.HTTPStatusError(
+                message=data.get("msg1", "KIS error"), request=r.request, response=r
+            )
+        return data
+
+    async def aclose(self):
+        await self._client.aclose()
diff --git a/packages/core/build/lib/core/clients/naver.py b/packages/core/build/lib/core/clients/naver.py
new file mode 100644
index 0000000..fe945ae
--- /dev/null
+++ b/packages/core/build/lib/core/clients/naver.py
@@ -0,0 +1,77 @@
+from __future__ import annotations
+import os
+import httpx
+from typing import Optional
+from datetime import datetime, timedelta
+from core.utils.cache import path, fresh, load_json, save_json
+
+NAVER_ID = os.getenv("NAVER_SEARCH_CLIENT_ID")
+NAVER_SECRET = os.getenv("NAVER_SEARCH_CLIENT_SECRET")
+
+
+async def company_logo(company_name: str, stock_code: str | None = None) -> str | None:
+    cache = path("logos", f"{company_name}.json")
+    if fresh(cache, days=30):
+        data = load_json(cache)
+        if data and data.get("logo_url") and data.get("logo_url") != "NO_LOGO":
+            return data["logo_url"]
+
+    if not stock_code or not NAVER_ID or not NAVER_SECRET:
+        return None
+
+    async with httpx.AsyncClient(timeout=5) as c:
+        r = await c.get(
+            "https://openapi.naver.com/v1/search/image",
+            headers={
+                "X-Naver-Client-Id": NAVER_ID,
+                "X-Naver-Client-Secret": NAVER_SECRET,
+            },
+            params={"query": f"{stock_code} tradingview", "display": 1, "sort": "sim"},
+        )
+        r.raise_for_status()
+        items = r.json().get("items", [])
+        logo_url = items[0]["link"] if items else "NO_LOGO"
+        save_json(
+            {
+                "company_name": company_name,
+                "logo_url": logo_url,
+                "expiry_date": (datetime.now() + timedelta(days=30)).isoformat(),
+            },
+            cache,
+        )
+        return None if logo_url == "NO_LOGO" else logo_url
+
+
+class NaverImageSearch:
+    def __init__(
+        self,
+        client_id: Optional[str],
+        client_secret: Optional[str],
+        *,
+        timeout: float = 5.0,
+    ):
+        self.client_id = client_id
+        self.client_secret = client_secret
+        self._client = httpx.AsyncClient(timeout=timeout)
+
+    def _enabled(self) -> bool:
+        return bool(self.client_id and self.client_secret)
+
+    async def search_one(self, query: str) -> str | None:
+        if not self._enabled():
+            # ìê²© ì—†ìœ¼ë©´ ì¡°ìš©íˆ None
+            return None
+        r = await self._client.get(
+            "https://openapi.naver.com/v1/search/image",
+            headers={
+                "X-Naver-Client-Id": self.client_id,
+                "X-Naver-Client-Secret": self.client_secret,
+            },
+            params={"query": query, "display": 1, "sort": "sim"},
+        )
+        # ë„¤ì´ë²„ APIëŠ” 200ì´ì–´ë„ items ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ
+        if r.status_code // 100 != 2:
+            return None
+        data = r.json()
+        items = data.get("items") or []
+        return items[0]["link"] if items else None
diff --git a/packages/core/build/lib/core/schemas/__init__.py b/packages/core/build/lib/core/schemas/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/packages/core/build/lib/core/schemas/financials.py b/packages/core/build/lib/core/schemas/financials.py
new file mode 100644
index 0000000..86e8bcf
--- /dev/null
+++ b/packages/core/build/lib/core/schemas/financials.py
@@ -0,0 +1,19 @@
+from pydantic import BaseModel
+from typing import List, Optional
+
+
+class FSRow(BaseModel):
+    account_id: Optional[str] = None
+    account_nm: Optional[str] = None
+    bsns_year: Optional[str] = None
+    thstrm_amount: Optional[str] = None
+    frmtrm_amount: Optional[str] = None
+    fs_div: Optional[str] = None
+    reprt_code: Optional[str] = None
+
+
+class FinancialStatement(BaseModel):
+    corp_code: str
+    year: int
+    report_name: Optional[str] = None
+    rows: List[FSRow] = []
diff --git a/packages/core/build/lib/core/schemas/prices.py b/packages/core/build/lib/core/schemas/prices.py
new file mode 100644
index 0000000..b1e19c8
--- /dev/null
+++ b/packages/core/build/lib/core/schemas/prices.py
@@ -0,0 +1,18 @@
+from pydantic import BaseModel
+from typing import List, Optional
+
+
+class PricePoint(BaseModel):
+    date: str
+    open: Optional[float] = None
+    high: Optional[float] = None
+    low: Optional[float] = None
+    close: Optional[float] = None
+    volume: Optional[float] = None
+    transaction_amount: Optional[float] = None
+    change: Optional[float] = None
+
+
+class PriceSeries(BaseModel):
+    ticker: str
+    points: List[PricePoint] = []
diff --git a/packages/core/build/lib/core/services/__init__.py b/packages/core/build/lib/core/services/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/packages/core/build/lib/core/services/analysis.py b/packages/core/build/lib/core/services/analysis.py
new file mode 100644
index 0000000..167bc50
--- /dev/null
+++ b/packages/core/build/lib/core/services/analysis.py
@@ -0,0 +1,302 @@
+from __future__ import annotations
+import logging
+from typing import Dict, Any
+import numpy as np
+import pandas as pd
+
+logger = logging.getLogger(__name__)
+
+# -----------------------------
+# Internal helpers
+# -----------------------------
+
+
+def _coerce_number(value: Any) -> float:
+    """Convert DART numeric strings like "1,234" â†’ float, else NaN."""
+    try:
+        if value is None or (isinstance(value, float) and np.isnan(value)):
+            return np.nan
+        s = str(value).replace(",", "").strip()
+        if s == "" or s.lower() == "nan":
+            return np.nan
+        return float(s)
+    except Exception:
+        return np.nan
+
+
+def _get_account_value(fs_df: pd.DataFrame, account_name: str) -> float:
+    """Safely extract `thstrm_amount` for an exact account name from DART FS df."""
+    if fs_df is None or fs_df.empty:
+        return np.nan
+    if "account_nm" not in fs_df.columns:
+        return np.nan
+    try:
+        row = fs_df[fs_df["account_nm"] == account_name]
+        if row.empty:
+            return np.nan
+        value = row.iloc[0].get("thstrm_amount")
+        return _coerce_number(value)
+    except Exception:
+        return np.nan
+
+
+# -----------------------------
+# Public API (pure functions)
+# -----------------------------
+
+
+def dcf_intrinsic_price(
+    *, fcf0: float, growth: float, wacc: float, terminal_g: float, shares: float
+) -> float:
+    """5Y DCF + Gordon terminal; returns price per share."""
+    years = np.arange(1, 6)
+    fcfs = fcf0 * (1.0 + growth) ** years
+    pv_fcfs = (fcfs / (1.0 + wacc) ** years).sum()
+    terminal_value = (fcfs[-1] * (1.0 + terminal_g)) / (wacc - terminal_g)
+    pv_terminal = terminal_value / (1.0 + wacc) ** 5
+    equity_value = pv_fcfs + pv_terminal
+    return float(equity_value / shares) if shares else float("nan")
+
+
+def rim_intrinsic_price(
+    *,
+    bps: float,
+    roe: float,
+    cost_of_equity: float,
+    growth: float,
+    years: int,
+    shares: float,
+) -> float:
+    """Residual Income Model with BV compounding, returns price per share."""
+    bv = float(bps)
+    residuals: list[float] = []
+    for t in range(years):
+        ni = bv * roe
+        bv = bv + ni - (bv * growth)  # retain earnings; simplistic payout modelling
+        ri = ni - (cost_of_equity * bv)
+        residuals.append(ri)
+    pv_residuals = sum(
+        ri / (1.0 + cost_of_equity) ** (i + 1) for i, ri in enumerate(residuals)
+    )
+    intrinsic = bps + pv_residuals
+    return float(intrinsic / shares) if shares else float("nan")
+
+
+def calculate_financial_health(fs_df: pd.DataFrame) -> Dict[str, float | str]:
+    """Compute debt/current ratios, ROE, op margin, interest coverage, Z-score-ish, score & grade."""
+    if fs_df is None or fs_df.empty:
+        return {
+            "debt_ratio": np.nan,
+            "current_ratio": np.nan,
+            "roe": np.nan,
+            "op_margin": np.nan,
+            "interest_coverage": np.nan,
+            "z_score": np.nan,
+            "total_score": np.nan,
+            "grade": "N/A",
+        }
+
+    total_assets = _get_account_value(fs_df, "ìì‚°ì´ê³„")
+    total_liabilities = _get_account_value(fs_df, "ë¶€ì±„ì´ê³„")
+    equity = _get_account_value(fs_df, "ìë³¸ì´ê³„")
+    current_assets = _get_account_value(fs_df, "ìœ ë™ìì‚°")
+    current_liabilities = _get_account_value(fs_df, "ìœ ë™ë¶€ì±„")
+    revenue = _get_account_value(fs_df, "ë§¤ì¶œì•¡")
+    operating_income = _get_account_value(fs_df, "ì˜ì—…ì´ìµ")
+    net_income = _get_account_value(fs_df, "ë‹¹ê¸°ìˆœì´ìµ")
+    interest_expense = _get_account_value(fs_df, "ì´ìë¹„ìš©")
+
+    debt_ratio = (total_liabilities / equity) * 100.0 if equity else np.nan
+    current_ratio = (
+        (current_assets / current_liabilities) * 100.0
+        if current_liabilities
+        else np.nan
+    )
+    roe = (net_income / equity) * 100.0 if equity else np.nan
+    op_margin = (operating_income / revenue) * 100.0 if revenue else np.nan
+    interest_coverage = (
+        (operating_income / interest_expense) if interest_expense else np.nan
+    )
+
+    def safe_div(a: float, b: float) -> float:
+        return a / b if (a is not None and b not in (None, 0, np.nan)) else np.nan
+
+    wca = safe_div(current_assets - current_liabilities, total_assets)
+    rea = safe_div(equity, total_assets)
+    eita = safe_div(operating_income, total_assets)
+    mvel = safe_div(equity, total_liabilities)
+    sta = safe_div(revenue, total_assets)
+
+    parts = [wca, rea, eita, mvel, sta]
+    z_score = (
+        (
+            (1.2 * (0.0 if np.isnan(wca) else wca))
+            + (1.4 * (0.0 if np.isnan(rea) else rea))
+            + (3.3 * (0.0 if np.isnan(eita) else eita))
+            + (0.6 * (0.0 if np.isnan(mvel) else mvel))
+            + (1.0 * (0.0 if np.isnan(sta) else sta))
+        )
+        if not all(np.isnan(x) for x in parts)
+        else np.nan
+    )
+
+    scores = {
+        "debt_ratio": max(
+            0.0, min(100.0, 100.0 - (debt_ratio if not np.isnan(debt_ratio) else 100.0))
+        ),
+        "current_ratio": max(
+            0.0, min(100.0, (current_ratio if not np.isnan(current_ratio) else 0.0))
+        ),
+        "roe": max(0.0, min(100.0, (roe if not np.isnan(roe) else 0.0))),
+        "op_margin": max(
+            0.0, min(100.0, (op_margin if not np.isnan(op_margin) else 0.0))
+        ),
+        "interest_coverage": max(
+            0.0,
+            min(
+                100.0,
+                (
+                    (interest_coverage * 10.0)
+                    if not np.isnan(interest_coverage)
+                    else 0.0
+                ),
+            ),
+        ),
+        "z_score": max(
+            0.0, min(100.0, ((z_score * 20.0) if not np.isnan(z_score) else 0.0))
+        ),
+    }
+
+    total_score = (
+        (
+            (scores["debt_ratio"] * 0.2)
+            + (scores["current_ratio"] * 0.2)
+            + (scores["roe"] * 0.2)
+            + (scores["op_margin"] * 0.15)
+            + (scores["interest_coverage"] * 0.15)
+            + (scores["z_score"] * 0.1)
+        )
+        if not all(np.isnan(v) for v in scores.values())
+        else np.nan
+    )
+
+    if np.isnan(total_score):
+        grade = "N/A"
+    elif total_score >= 80.0:
+        grade = "A"
+    elif total_score >= 60.0:
+        grade = "B"
+    else:
+        grade = "C"
+
+    return {
+        "debt_ratio": float(debt_ratio) if not np.isnan(debt_ratio) else np.nan,
+        "current_ratio": (
+            float(current_ratio) if not np.isnan(current_ratio) else np.nan
+        ),
+        "roe": float(roe) if not np.isnan(roe) else np.nan,
+        "op_margin": float(op_margin) if not np.isnan(op_margin) else np.nan,
+        "interest_coverage": (
+            float(interest_coverage) if not np.isnan(interest_coverage) else np.nan
+        ),
+        "z_score": float(z_score) if not np.isnan(z_score) else np.nan,
+        "total_score": float(total_score) if not np.isnan(total_score) else np.nan,
+        "grade": grade,
+    }
+
+
+def calculate_custom_ratios(
+    fs_df: pd.DataFrame, price_df: pd.DataFrame
+) -> Dict[str, float]:
+    """Compute PER, PBR, dividend yield from FS + latest price."""
+    if fs_df is None or fs_df.empty or price_df is None or price_df.empty:
+        return {"PER": np.nan, "PBR": np.nan, "ë°°ë‹¹ìˆ˜ìµë¥ (%)": np.nan}
+
+    latest = price_df["close"].dropna()
+    latest_price = float(latest.iloc[-1]) if not latest.empty else np.nan
+    if np.isnan(latest_price):
+        return {"PER": np.nan, "PBR": np.nan, "ë°°ë‹¹ìˆ˜ìµë¥ (%)": np.nan}
+
+    shares_outstanding = _get_account_value(fs_df, "ë°œí–‰ì£¼ì‹ìˆ˜")
+    net_income = _get_account_value(fs_df, "ë‹¹ê¸°ìˆœì´ìµ")
+    total_equity = _get_account_value(fs_df, "ìë³¸ì´ê³„")
+    dividends = _get_account_value(fs_df, "ë°°ë‹¹ê¸ˆì´ì•¡")
+
+    market_cap = (
+        latest_price * shares_outstanding
+        if (not np.isnan(latest_price) and not np.isnan(shares_outstanding))
+        else np.nan
+    )
+    per = (
+        (market_cap / net_income)
+        if (not np.isnan(market_cap) and not np.isnan(net_income) and net_income != 0)
+        else np.nan
+    )
+    pbr = (
+        (market_cap / total_equity)
+        if (
+            not np.isnan(market_cap)
+            and not np.isnan(total_equity)
+            and total_equity != 0
+        )
+        else np.nan
+    )
+    dy = (
+        ((dividends / market_cap) * 100.0)
+        if (not np.isnan(market_cap) and not np.isnan(dividends) and market_cap != 0)
+        else np.nan
+    )
+
+    return {
+        "PER": round(float(per), 2) if not np.isnan(per) else np.nan,
+        "PBR": round(float(pbr), 2) if not np.isnan(pbr) else np.nan,
+        "ë°°ë‹¹ìˆ˜ìµë¥ (%)": round(float(dy), 2) if not np.isnan(dy) else np.nan,
+    }
+
+
+def compare_by_industry(df: pd.DataFrame) -> pd.DataFrame:
+    """Return simple mean of numeric columns grouped by 'ì—…ì¢…'."""
+    if df is None or df.empty or "ì—…ì¢…" not in df.columns:
+        return pd.DataFrame()
+    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
+    if "ì—…ì¢…" in numeric_cols:
+        numeric_cols.remove("ì—…ì¢…")
+    if not numeric_cols:
+        return pd.DataFrame()
+    return df.groupby("ì—…ì¢…")[numeric_cols].mean()
+
+
+def extract_fs_summary(fs_df: pd.DataFrame) -> Dict[str, float]:
+    """Pick a few headline FS metrics for a card view."""
+    if fs_df is None or fs_df.empty:
+        return {
+            "ë§¤ì¶œì•¡": np.nan,
+            "ì˜ì—…ì´ìµ": np.nan,
+            "ë‹¹ê¸°ìˆœì´ìµ": np.nan,
+            "ì´ìì‚°": np.nan,
+            "ì´ë¶€ì±„": np.nan,
+            "ìë³¸ì´ê³„": np.nan,
+        }
+
+    def pick(key: str) -> float:
+        # use exact match first; fallback to contains
+        v = _get_account_value(fs_df, key)
+        if np.isnan(v):
+            try:
+                row = fs_df[
+                    fs_df["account_nm"].str.contains(key, na=False, regex=False)
+                ]
+                if not row.empty:
+                    return _coerce_number(row.iloc[0].get("thstrm_amount"))
+            except Exception:
+                pass
+        return v
+
+    return {
+        "ë§¤ì¶œì•¡": pick("ë§¤ì¶œì•¡"),
+        "ì˜ì—…ì´ìµ": pick("ì˜ì—…ì´ìµ"),
+        "ë‹¹ê¸°ìˆœì´ìµ": pick("ë‹¹ê¸°ìˆœì´ìµ"),
+        "ì´ìì‚°": pick("ìì‚°ì´ê³„"),
+        "ì´ë¶€ì±„": pick("ë¶€ì±„ì´ê³„"),
+        "ìë³¸ì´ê³„": pick("ìë³¸ì´ê³„"),
+    }
diff --git a/packages/core/build/lib/core/services/logo.py b/packages/core/build/lib/core/services/logo.py
new file mode 100644
index 0000000..6f165da
--- /dev/null
+++ b/packages/core/build/lib/core/services/logo.py
@@ -0,0 +1,41 @@
+from __future__ import annotations
+from datetime import datetime, timedelta
+from core.utils.cache import path, fresh, save_json, load_json
+from core.clients.naver import NaverImageSearch
+
+
+async def get_logo_cached(
+    naver: NaverImageSearch, *, company_name: str, stock_code: str | None = None
+) -> dict:
+    key = stock_code or company_name
+    cache_file = path("logos", f"{key}.json")
+
+    # 30ì¼ ìºì‹œ
+    if fresh(cache_file, days=30):
+        cached = load_json(cache_file)
+        if cached is not None:
+            return cached
+
+    # ìê²© ì—†ìœ¼ë©´ graceful degrade
+    if not (naver and naver.client_id and naver.client_secret):
+        data = {
+            "company": company_name,
+            "stock_code": stock_code,
+            "logo_url": None,
+            "cached": False,
+            "reason": "missing_naver_credentials",
+        }
+        save_json(data, cache_file)
+        return data
+
+    # ê²€ìƒ‰ ì¿¼ë¦¬: ìš°ì„  ì¢…ëª©ì½”ë“œ, ì—†ìœ¼ë©´ íšŒì‚¬ëª…
+    query = f"{stock_code} tradingview" if stock_code else f"{company_name} ë¡œê³ "
+    url = await naver.search_one(query)
+    data = {
+        "company": company_name,
+        "stock_code": stock_code,
+        "logo_url": url,
+        "cached": False,
+    }
+    save_json(data, cache_file)
+    return data
diff --git a/packages/core/build/lib/core/services/lookup.py b/packages/core/build/lib/core/services/lookup.py
new file mode 100644
index 0000000..be2cf59
--- /dev/null
+++ b/packages/core/build/lib/core/services/lookup.py
@@ -0,0 +1,75 @@
+from __future__ import annotations
+import io, zipfile, os
+import pandas as pd
+import httpx
+from fastapi import HTTPException
+from core.utils.cache import path, fresh, save_parquet, load_parquet
+
+
+async def corp_table(api_key: str) -> pd.DataFrame:
+    """
+    Fetch DART corpCode.zip (XML inside) safely.
+    - Requires api_key (do NOT read env here).
+    - Falls back to cached parquet if DART returns non-zip payloads.
+    """
+    cache_file = path("corp_codes", "corp_code_list.parquet")
+    if fresh(cache_file, days=1):
+        df = load_parquet(cache_file)
+        if df is not None:
+            return df
+
+    url = f"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={api_key}"
+    async with httpx.AsyncClient(timeout=25) as c:
+        r = await c.get(url)
+        r.raise_for_status()
+
+        # Guard: DART sometimes returns text (error) with 200
+        ctype = (r.headers.get("Content-Type") or "").lower()
+        is_zipish = "zip" in ctype or r.content[:2] == b"PK"
+        if not is_zipish:
+            # Fallback to cache if any
+            cached = load_parquet(cache_file)
+            if cached is not None:
+                return cached
+            snippet = r.text[:200].replace("\n", " ")
+            raise HTTPException(
+                status_code=502,
+                detail=f"DART corpCode not zip; response hint: {snippet}",
+            )
+
+        try:
+            with zipfile.ZipFile(io.BytesIO(r.content)) as z:
+                with z.open("CORPCODE.xml") as f:
+                    import xml.etree.ElementTree as ET
+
+                    tree = ET.parse(f)
+        except zipfile.BadZipFile as e:
+            cached = load_parquet(cache_file)
+            if cached is not None:
+                return cached
+            raise HTTPException(status_code=502, detail=f"DART zip parse failed: {e}")
+
+    root = tree.getroot()
+    rows = []
+    for e in root.findall("list"):
+        stock = (e.findtext("stock_code") or "").strip()
+        if not stock:
+            continue
+        rows.append((e.findtext("corp_code"), e.findtext("corp_name"), stock))
+    df = pd.DataFrame(rows, columns=["corp_code", "corp_name", "stock_code"])
+    df["stock_code"] = df["stock_code"].astype(str).str.zfill(6)
+    save_parquet(df, cache_file)
+    return df
+
+
+async def company_info_by_stock(stock_code: str, api_key: str) -> dict | None:
+    df = await corp_table(api_key)
+    row = df[df["stock_code"] == stock_code]
+    if row.empty:
+        return None
+    r = row.iloc[0]
+    return {
+        "corp_name": r["corp_name"],
+        "corp_code": r["corp_code"],
+        "stock_code": r["stock_code"],
+    }
diff --git a/packages/core/build/lib/core/services/market_data.py b/packages/core/build/lib/core/services/market_data.py
new file mode 100644
index 0000000..1c7c807
--- /dev/null
+++ b/packages/core/build/lib/core/services/market_data.py
@@ -0,0 +1,191 @@
+from __future__ import annotations
+import pandas as pd
+import asyncio
+from core.clients.kis import KISClient
+from core.clients.dart import DARTClient
+from core.utils.cache import path, fresh, save_parquet, load_parquet
+from core.schemas.financials import FinancialStatement, FSRow
+
+
+# ------------------
+# KIS: daily price
+# ------------------
+async def kis_daily_price(
+    kis: KISClient, stock_code: str, start_date: str, end_date: str
+) -> pd.DataFrame:
+    start_dt = pd.to_datetime(start_date)
+    end_dt = pd.to_datetime(end_date)
+    cache_file = path(
+        "prices", stock_code, f"kis_{start_dt:%Y%m%d}_{end_dt:%Y%m%d}.parquet"
+    )
+    if fresh(cache_file, days=1):
+        cached = load_parquet(cache_file)
+        if cached is not None:
+            return cached
+
+    data = await kis.get(
+        "/uapi/domestic-stock/v1/quotations/inquire-daily-price",
+        tr_id="FHKST01010400",
+        params={
+            "FID_COND_MRKT_DIV_CODE": "J",
+            "FID_INPUT_ISCD": stock_code,
+            "FID_INPUT_DATE_1": f"{start_dt:%Y%m%d}",
+            "FID_INPUT_DATE_2": f"{end_dt:%Y%m%d}",
+            "FID_PERIOD_DIV_CODE": "D",
+            "FID_ORG_ADJ_PRC": "1",
+        },
+    )
+
+    df = pd.DataFrame(data.get("output", []))
+    if df.empty:
+        return df
+
+    df = df.rename(
+        columns={
+            "stck_bsop_date": "date",
+            "stck_oprc": "open",
+            "stck_hgpr": "high",
+            "stck_lwpr": "low",
+            "stck_clpr": "close",
+            "acml_vol": "volume",
+            "acml_tr_pbmn": "transaction_amount",
+            "prdy_vrss": "change",
+        }
+    )
+
+    cols = [
+        c
+        for c in [
+            "date",
+            "open",
+            "high",
+            "low",
+            "close",
+            "volume",
+            "transaction_amount",
+            "change",
+        ]
+        if c in df.columns
+    ]
+    for c in cols:
+        if c != "date":
+            df[c] = pd.to_numeric(df[c], errors="coerce")
+    df["date"] = pd.to_datetime(df["date"], format="%Y%m%d").dt.strftime("%Y-%m-%d")
+    df = df[cols].sort_values("date").reset_index(drop=True)
+    save_parquet(df, cache_file)
+    return df
+
+
+# ------------------
+# DART: financials
+# ------------------
+REPORTS = [
+    ("11011", "ì‚¬ì—…ë³´ê³ ì„œ"),
+    ("11014", "3ë¶„ê¸°ë³´ê³ ì„œ"),
+    ("11012", "ë°˜ê¸°ë³´ê³ ì„œ"),
+    ("11013", "1ë¶„ê¸°ë³´ê³ ì„œ"),
+]
+FSDIVS = [("CFS", "ì—°ê²°"), ("OFS", "ë³„ë„")]
+
+
+async def dart_financials(
+    dart: DARTClient, corp_code: str, year: int
+) -> FinancialStatement | None:
+    """Return the first available FS for (corp_code, year) with a friendly report name.
+    Caches raw rows as parquet for 7 days.
+    """
+    for rp_code, rp_name in REPORTS:
+        for fs_div, fs_name in FSDIVS:
+            cache_file = path(
+                "financials", corp_code, f"{year}_{rp_code}_{fs_div}.parquet"
+            )
+            if fresh(cache_file, days=7):
+                cached = load_parquet(cache_file)
+                if cached is not None:
+                    return FinancialStatement(
+                        corp_code=corp_code,
+                        year=year,
+                        report_name=f"{rp_name} - {fs_name}",
+                        rows=[FSRow(**r) for r in cached.to_dict(orient="records")],
+                    )
+
+            data = await dart.single_fs(corp_code, year, rp_code, fs_div)
+            if data.get("status") == "000" and data.get("list"):
+                df = pd.DataFrame(data["list"])  # store raw
+                save_parquet(df, cache_file)
+                return FinancialStatement(
+                    corp_code=corp_code,
+                    year=year,
+                    report_name=f"{rp_name} - {fs_name}",
+                    rows=[FSRow(**r) for r in data["list"]],
+                )
+    return None
+
+
+async def kis_prices_panel(
+    kis: KISClient, stock_codes: list[str], start_date: str, end_date: str
+) -> pd.DataFrame:
+    async def one(code: str) -> pd.Series:
+        df = await kis_daily_price(kis, code, start_date, end_date)
+        if df is None or df.empty:
+            return pd.Series(name=code, dtype=float)
+        s = df.set_index("date")["close"].astype(float)
+        s.index = pd.to_datetime(s.index)
+        s.name = code
+        return s
+
+    series_list = await asyncio.gather(*[one(c) for c in stock_codes])
+    panel = pd.concat(series_list, axis=1).sort_index()
+    returns = panel.pct_change().dropna(how="all")
+    return returns
+
+
+async def kis_financial_ratios(kis: KISClient, stock_code: str) -> pd.DataFrame:
+    data = await kis.get(
+        "/uapi/domestic-stock/v1/finance/financial-ratio",
+        tr_id="FHKST66430300",
+        params={
+            "fid_div_cls_code": "0",
+            "fid_cond_mrkt_div_code": "J",
+            "fid_input_iscd": stock_code,
+        },
+    )
+    df = pd.DataFrame(data.get("output", []))
+    if df.empty:
+        return df
+    cols = {
+        "stac_yymm": "ê²°ì‚°ë…„ì›”",
+        "grs_rt": "ë§¤ì¶œì´ì´ìµë¥ ",
+        "bsop_prfi_inrt": "ì˜ì—…ì´ìµë¥ ",
+        "thtr_ntin_inrt": "ë‹¹ê¸°ìˆœì´ìµë¥ ",
+        "roe_val": "ROE",
+        "eps": "EPS",
+        "bps": "BPS",
+        "pbr": "PBR",
+        "dvd_yd_rt": "ë°°ë‹¹ìˆ˜ìµë¥ ",
+    }
+    df = df[[k for k in cols if k in df.columns]].rename(columns=cols)
+    for c in df.columns:
+        if c != "ê²°ì‚°ë…„ì›”":
+            df[c] = pd.to_numeric(df[c], errors="coerce")
+    return df
+
+
+async def kis_investment_opinion(kis: KISClient, stock_code: str) -> dict:
+    data = await kis.get(
+        "/uapi/domestic-stock/v1/quotations/invest-opinion",
+        tr_id="FHKST663300C0",
+        params={
+            "FID_COND_MRKT_DIV_CODE": "J",
+            "FID_COND_SCR_DIV_CODE": "16633",
+            "FID_INPUT_ISCD": stock_code,
+            "FID_INPUT_DATE_1": pd.Timestamp.today().strftime("%Y0101"),
+            "FID_INPUT_DATE_2": pd.Timestamp.today().strftime("%Y%m%d"),
+        },
+    )
+    out = (data.get("output") or [{}])[0]
+    return {
+        "opinion": out.get("invt_opnn"),
+        "target_price": float(out.get("hts_goal_prc") or 0.0),
+        "analyst_count": int(out.get("nm_of_analyst") or 0),
+    }
diff --git a/packages/core/build/lib/core/services/metrics.py b/packages/core/build/lib/core/services/metrics.py
new file mode 100644
index 0000000..ddb5f88
--- /dev/null
+++ b/packages/core/build/lib/core/services/metrics.py
@@ -0,0 +1,90 @@
+from __future__ import annotations
+import numpy as np
+import pandas as pd
+
+# Safe extract (matches legacy semantics)
+
+
+def extract_value(df: pd.DataFrame | None, account_name: str) -> float:
+    if df is None or df.empty:
+        return float("nan")
+    row = df[df["account_nm"] == account_name]
+    if row.empty:
+        return float("nan")
+    return float(pd.to_numeric(row.iloc[0].get("thstrm_amount"), errors="coerce"))
+
+
+def calculate_custom_metrics(
+    df_fs: pd.DataFrame | None, df_price: pd.DataFrame | None
+) -> dict:
+    if df_fs is None or df_fs.empty:
+        return {}
+    m = {
+        "revenue": extract_value(df_fs, "ë§¤ì¶œì•¡"),
+        "operating_income": extract_value(df_fs, "ì˜ì—…ì´ìµ"),
+        "net_income": extract_value(df_fs, "ë‹¹ê¸°ìˆœì´ìµ"),
+        "total_assets": extract_value(df_fs, "ìì‚°ì´ê³„"),
+        "total_liabilities": extract_value(df_fs, "ë¶€ì±„ì´ê³„"),
+        "total_equity": extract_value(df_fs, "ìë³¸ì´ê³„"),
+    }
+    if df_price is not None and not df_price.empty:
+        m["latest_close_price"] = float(
+            pd.to_numeric(df_price["close"].iloc[-1], errors="coerce")
+        )
+    return m
+
+
+# Piotroski F-score
+
+
+def calculate_piotroski_f_score(
+    df_curr: pd.DataFrame | None, df_prev: pd.DataFrame | None
+) -> tuple[int, dict]:
+    MAP = {
+        "NI": "ë‹¹ê¸°ìˆœì´ìµ",
+        "CFO": "ì˜ì—…í™œë™ìœ¼ë¡œì¸í•œí˜„ê¸ˆíë¦„",
+        "TA": "ìì‚°ì´ê³„",
+        "TL": "ë¶€ì±„ì´ê³„",
+        "CA": "ìœ ë™ìì‚°",
+        "CL": "ìœ ë™ë¶€ì±„",
+        "REV": "ë§¤ì¶œì•¡",
+        "COGS": "ë§¤ì¶œì›ê°€",
+        "SHARES": "ìœ í†µì£¼ì‹ìˆ˜",
+    }
+
+    def v(df: pd.DataFrame | None, key: str) -> float:
+        if df is None or df.empty:
+            return float("nan")
+        row = df[df["account_nm"] == MAP[key]]
+        return (
+            float(pd.to_numeric(row["thstrm_amount"].iloc[0], errors="coerce"))
+            if not row.empty
+            else float("nan")
+        )
+
+    if df_curr is None or df_prev is None:
+        return 0, {}
+
+    detail: dict[str, int] = {}
+    roa_c = v(df_curr, "NI") / v(df_curr, "TA")
+    detail["1. ROA > 0"] = int(roa_c > 0)
+    cfo_c = v(df_curr, "CFO")
+    detail["2. CFO > 0"] = int(cfo_c > 0)
+    roa_p = v(df_prev, "NI") / v(df_prev, "TA")
+    detail["3. ROA ì¦ê°€"] = int(roa_c > roa_p)
+    detail["4. CFO > NI"] = int(cfo_c > v(df_curr, "NI"))
+    lev_c = v(df_curr, "TL") / v(df_curr, "TA")
+    lev_p = v(df_prev, "TL") / v(df_prev, "TA")
+    detail["5. ë ˆë²„ë¦¬ì§€ ë¹„ìœ¨ ê°ì†Œ"] = int(lev_c < lev_p)
+    cr_c = v(df_curr, "CA") / v(df_curr, "CL")
+    cr_p = v(df_prev, "CA") / v(df_prev, "CL")
+    detail["6. ìœ ë™ë¹„ìœ¨ ì¦ê°€"] = int(cr_c > cr_p)
+    detail["7. ì‹ ì£¼ë°œí–‰ ì—†ìŒ"] = int(v(df_curr, "SHARES") <= v(df_prev, "SHARES"))
+    gm_c = (v(df_curr, "REV") - v(df_curr, "COGS")) / v(df_curr, "REV")
+    gm_p = (v(df_prev, "REV") - v(df_prev, "COGS")) / v(df_prev, "REV")
+    detail["8. ì´ì´ìµë¥  ì¦ê°€"] = int(gm_c > gm_p)
+    at_c = v(df_curr, "REV") / v(df_curr, "TA")
+    at_p = v(df_prev, "REV") / v(df_prev, "TA")
+    detail["9. ìì‚°íšŒì „ìœ¨ ì¦ê°€"] = int(at_c > at_p)
+
+    return int(sum(detail.values())), detail
diff --git a/packages/core/build/lib/core/services/portfolio.py b/packages/core/build/lib/core/services/portfolio.py
new file mode 100644
index 0000000..45d85ba
--- /dev/null
+++ b/packages/core/build/lib/core/services/portfolio.py
@@ -0,0 +1,158 @@
+from __future__ import annotations
+import numpy as np
+import pandas as pd
+from scipy.optimize import minimize
+
+ANNUALIZATION_FACTOR = 252
+DEFAULT_RISK_FREE_RATE = 0.02
+
+# ----------------- helpers -----------------
+
+
+def _annualized(
+    mu_daily: np.ndarray, cov_daily: np.ndarray
+) -> tuple[np.ndarray, np.ndarray]:
+    return mu_daily * ANNUALIZATION_FACTOR, cov_daily * ANNUALIZATION_FACTOR
+
+
+# API-safe version of legacy calculate_portfolio_performance
+
+
+def calculate_portfolio_performance(
+    weights: np.ndarray, mean_returns: pd.Series, cov_matrix: pd.DataFrame
+) -> tuple[float, float]:
+    if (
+        not isinstance(weights, np.ndarray)
+        or weights.ndim != 1
+        or len(weights) != len(mean_returns)
+    ):
+        return np.nan, np.nan
+    if mean_returns.empty or cov_matrix.empty:
+        return np.nan, np.nan
+    try:
+        mu_ann, cov_ann = _annualized(mean_returns.to_numpy(), cov_matrix.to_numpy())
+        ret = float(np.sum(mu_ann * weights))
+        vol = float(np.sqrt(weights.T @ cov_ann @ weights))
+        return vol, ret
+    except Exception:
+        return np.nan, np.nan
+
+
+# legacy-compatible negative Sharpe
+
+
+def negative_sharpe_ratio(
+    weights: np.ndarray,
+    mean_returns: pd.Series,
+    cov_matrix: pd.DataFrame,
+    risk_free_rate: float = DEFAULT_RISK_FREE_RATE,
+) -> float:
+    vol, ret = calculate_portfolio_performance(weights, mean_returns, cov_matrix)
+    if np.isnan(vol) or vol == 0:
+        return np.inf
+    return -((ret - risk_free_rate) / vol)
+
+
+# ----------------- optimizers -----------------
+
+
+def optimize_portfolio(
+    returns_df: pd.DataFrame, risk_free_rate: float = DEFAULT_RISK_FREE_RATE
+) -> dict:
+    if returns_df is None or returns_df.empty:
+        return {
+            "weights": None,
+            "annual_return": np.nan,
+            "annual_volatility": np.nan,
+            "sharpe_ratio": np.nan,
+            "success": False,
+        }
+
+    mean_returns = returns_df.mean()
+    cov_matrix = returns_df.cov()
+    n = len(mean_returns)
+
+    constraints = {"type": "eq", "fun": lambda x: np.sum(x) - 1}
+    bounds = tuple((0.0, 1.0) for _ in range(n))  # long-only
+    x0 = np.full(n, 1.0 / n)
+
+    res = minimize(
+        negative_sharpe_ratio,
+        x0,
+        args=(mean_returns, cov_matrix, risk_free_rate),
+        method="SLSQP",
+        bounds=bounds,
+        constraints=constraints,
+        options={"maxiter": 100, "ftol": 1e-9},
+    )
+
+    if not res.success:
+        return {
+            "weights": None,
+            "annual_return": np.nan,
+            "annual_volatility": np.nan,
+            "sharpe_ratio": np.nan,
+            "success": False,
+        }
+
+    w = np.asarray(res.x, dtype=float)
+    vol, ret = calculate_portfolio_performance(w, mean_returns, cov_matrix)
+    sharpe = float((ret - risk_free_rate) / vol) if vol else np.nan
+
+    return {
+        "weights": w,
+        "annual_return": ret,
+        "annual_volatility": vol,
+        "sharpe_ratio": sharpe,
+        "success": True,
+    }
+
+
+# ----------------- backtest -----------------
+
+
+def backtest_portfolio(price_data: pd.DataFrame, weights: np.ndarray) -> dict:
+    if price_data is None or price_data.empty or weights is None or len(weights) == 0:
+        return {
+            "cumulative_returns": pd.Series(dtype=float),
+            "annual_return": np.nan,
+            "annual_volatility": np.nan,
+            "sharpe_ratio": np.nan,
+            "max_drawdown": np.nan,
+        }
+
+    price = price_data.sort_index().ffill().bfill()
+    rets = price.pct_change().dropna()
+    if rets.empty or len(weights) != rets.shape[1]:
+        return {
+            "cumulative_returns": pd.Series(dtype=float),
+            "annual_return": np.nan,
+            "annual_volatility": np.nan,
+            "sharpe_ratio": np.nan,
+            "max_drawdown": np.nan,
+        }
+
+    port_rets = (rets * weights).sum(axis=1)
+    cum = (1.0 + port_rets).cumprod()
+
+    ann_ret = (
+        float(cum.iloc[-1] ** (ANNUALIZATION_FACTOR / len(rets)) - 1.0)
+        if len(rets)
+        else np.nan
+    )
+    ann_vol = (
+        float(port_rets.std() * np.sqrt(ANNUALIZATION_FACTOR)) if len(rets) else np.nan
+    )
+    sharpe = float((ann_ret - DEFAULT_RISK_FREE_RATE) / ann_vol) if ann_vol else np.nan
+
+    peak = cum.expanding(min_periods=1).max()
+    dd = (cum - peak) / peak
+    mdd = float(dd.min()) if not dd.empty else np.nan
+
+    return {
+        "cumulative_returns": cum,
+        "annual_return": ann_ret,
+        "annual_volatility": ann_vol,
+        "sharpe_ratio": sharpe,
+        "max_drawdown": mdd,
+    }
diff --git a/packages/core/build/lib/core/utils/__init__.py b/packages/core/build/lib/core/utils/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/packages/core/build/lib/core/utils/cache.py b/packages/core/build/lib/core/utils/cache.py
new file mode 100644
index 0000000..e680b33
--- /dev/null
+++ b/packages/core/build/lib/core/utils/cache.py
@@ -0,0 +1,50 @@
+from __future__ import annotations
+import os, json
+from datetime import datetime, timedelta
+import pandas as pd
+from typing import Any
+
+BASE = os.getenv("CORE_CACHE_DIR", os.path.join("data", "cache"))
+
+
+def path(*parts: str) -> str:
+    p = os.path.join(BASE, *parts)
+    os.makedirs(os.path.dirname(p), exist_ok=True)
+    return p
+
+
+def fresh(p: str, days: int = 1) -> bool:
+    if not os.path.exists(p):
+        return False
+    mt = datetime.fromtimestamp(os.path.getmtime(p))
+    return (datetime.now() - mt) < timedelta(days=days)
+
+
+# JSON helpers
+
+
+def save_json(obj: Any, p: str) -> None:
+    with open(p, "w", encoding="utf-8") as f:
+        json.dump(obj, f, ensure_ascii=False, indent=2)
+
+
+def load_json(p: str) -> Any | None:
+    try:
+        with open(p, "r", encoding="utf-8") as f:
+            return json.load(f)
+    except Exception:
+        return None
+
+
+# Parquet helpers
+
+
+def save_parquet(df: pd.DataFrame, p: str) -> None:
+    df.to_parquet(p, index=False)
+
+
+def load_parquet(p: str) -> pd.DataFrame | None:
+    try:
+        return pd.read_parquet(p)
+    except Exception:
+        return None
diff --git a/services/api/financial_api.egg-info/PKG-INFO b/services/api/financial_api.egg-info/PKG-INFO
index d4f4c7b..b7991a3 100644
--- a/services/api/financial_api.egg-info/PKG-INFO
+++ b/services/api/financial_api.egg-info/PKG-INFO
@@ -5,4 +5,4 @@ Requires-Python: >=3.10
 Requires-Dist: fastapi>=0.112
 Requires-Dist: uvicorn[standard]>=0.30
 Requires-Dist: python-dotenv>=1.0
-Requires-Dist: financial-core@ file:///${PROJECT_ROOT}/packages/core
+Requires-Dist: financial-core@ file:///C:/workspace/kiwoom-python/financial_project/packages/core
diff --git a/services/api/financial_api.egg-info/requires.txt b/services/api/financial_api.egg-info/requires.txt
index a905355..c82cfe9 100644
--- a/services/api/financial_api.egg-info/requires.txt
+++ b/services/api/financial_api.egg-info/requires.txt
@@ -1,4 +1,4 @@
 fastapi>=0.112
 uvicorn[standard]>=0.30
 python-dotenv>=1.0
-financial-core@ file:///${PROJECT_ROOT}/packages/core
+financial-core@ file:///C:/workspace/kiwoom-python/financial_project/packages/core
diff --git a/services/api/pyproject.toml b/services/api/pyproject.toml
index d663b3f..d03444e 100644
--- a/services/api/pyproject.toml
+++ b/services/api/pyproject.toml
@@ -10,9 +10,20 @@ dependencies = [
   "fastapi>=0.112",
   "uvicorn[standard]>=0.30",
   "python-dotenv>=1.0",
-  "financial-core @ file:///${PROJECT_ROOT}/packages/core"
+  "financial-core @ file:///C:/workspace/kiwoom-python/financial_project/packages/core"
 ]
 
 [tool.setuptools]
 packages = ["app"]
-include-package-data = true
\ No newline at end of file
+include-package-data = true
+
+[tool.ruff]
+line-length = 100
+exclude = [
+  "packages/core/build",
+  "dist",
+  "*.egg-info",
+]
+
+[tool.ruff.lint]
+select = ["E", "F", "I"]
\ No newline at end of file
